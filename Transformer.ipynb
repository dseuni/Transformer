{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "purple-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # cuda 사용시 check 필요1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satisfactory-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-integral",
   "metadata": {},
   "source": [
    "## real dataset sample 생성하기\n",
    "\n",
    "- QA Test <br>\n",
    "    https://towardsdatascience.com/question-answering-with-pretrained-transformers-using-pytorch-c3e7a44b4012 <br>\n",
    "    https://medium.com/@patonw/question-answering-with-pytorch-transformers-part-1-8736196bf20e <br>\n",
    "    https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/ <br>\n",
    "    \n",
    "    - dataset <br>\n",
    "        : Question, Context -> Answer <br>\n",
    "         [START] QuestionContext(word_token) + 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 (type_token) + (position_token) <br>\n",
    "         [START] Answer(word_token) [END]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-solid",
   "metadata": {},
   "source": [
    "### BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "described-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fewer-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.BERT_PRETRAINED_CONFIG_ARCHIVE_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polished-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "original-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in dir(transformers) if 'BERT' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liquid-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4931, 102, 3828, 102], 'token_type_ids': [0, 0, 0, 1, 1], 'special_tokens_mask': [1, 0, 1, 0, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus('hey','save', return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "premium-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-ministry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "entitled-flush",
   "metadata": {},
   "source": [
    "### SQuAD\n",
    "- BERT에서 test했던 QA 데이터셋\n",
    "    https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worth-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('train-v2.0.json', 'r') as f:\n",
    "    squad_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "perceived-sherman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'data'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "angry-emperor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v2.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spiritual-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(squad_train['data']), len(squad_train['data'][0]['paragraphs']), squad_train['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aggregate-sapphire",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# squad_train['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automatic-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [03:19<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "ids_dic = {}\n",
    "dataset = []\n",
    "for data in tqdm(squad_train['data']):\n",
    "    for paragraphs in data['paragraphs']:\n",
    "        context = paragraphs['context']\n",
    "        questions = []\n",
    "        anwers = []\n",
    "        for gas in paragraphs['qas']:\n",
    "            if gas['is_impossible'] == True: # classification 같이 할 때 고민\n",
    "                continue\n",
    "            ## padding 해줘야할까?\n",
    "            # {'input_ids': [101, 4931, 102, 3828, 102], 'token_type_ids': [0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
    "            X = tokenizer.encode_plus(gas['question'], context, padding='max_length', truncation=True, max_length=128)\n",
    "            Y = tokenizer.encode_plus(gas['answers'][0]['text'], padding='max_length', truncation=True, max_length=32)\n",
    "            idx = gas['id']\n",
    "            ids_dic[idx] = [X, Y]\n",
    "            dataset.append(idx)\n",
    "#             break\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "backed-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(ids_dic.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compact-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_dic['56be85543aeaaa14008c9063']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "handy-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset[int(len(dataset)*0.9):]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dataset[:int(len(dataset)*0.9)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "productive-valley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.keys()#, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "congressional-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "miniature-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(idx):\n",
    "#     print(tf.compat.as_str_any(idx))\n",
    "    inp, tar = ids_dic[tf.compat.as_str_any(idx)]\n",
    "    return tf.cast(list(inp.values()), dtype=tf.float32) , tf.cast(list(tar.values()), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-definition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dominant-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda x: tf.numpy_function(\n",
    "                        map_func, [x], [tf.float32, tf.float32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "demographic-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.map(lambda x: tf.numpy_function(\n",
    "                        map_func, [x], [tf.float32, tf.float32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prostate-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.repeat(16).shuffle(BUFFER_SIZE).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fatty-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "healthy-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "natural-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138928"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "revised-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, tar in train_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-rabbit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "protective-developer",
   "metadata": {},
   "source": [
    "# euni transformer test\n",
    "- 참고: \n",
    "    - https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "    - https://wikidocs.net/31379\n",
    "    - https://www.tensorflow.org/text/tutorials/transformer\n",
    "    - https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "\n",
    "## 0. functions\n",
    "    - positional_encoding\n",
    "    - attention (scaled-dot: transformer에서 사용)\n",
    "        - multi-head\n",
    "        - padding mask\n",
    "        - 다음 token masking\n",
    "\n",
    "    (이미 존재)\n",
    "    - embedding\n",
    "    - position-wise FFNN\n",
    "    - layer norm (& residual)\n",
    "    \n",
    "## 1. encoder\n",
    "## 2. decoder\n",
    "    - encoder-decoder attention\n",
    "    \n",
    "## 3. Transformer\n",
    "\n",
    "## 4. ETC\n",
    "    - masking function\n",
    "        - padding mask\n",
    "        - 다음 token masking\n",
    "    - metric\n",
    "        - loss\n",
    "        - accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-officer",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "prepared-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        # The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.\n",
    "        # where pos is the position and i is the dimension.\n",
    "        return pos / 100000**((2*(i//2)/ np.float32(d_model)))\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        \n",
    "        # np.newaxis: np.array dimension을 늘려줌\n",
    "        # angle_rads.shape == (postion, d_model)\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                np.arange(d_model)[np.newaxis, :],\n",
    "                                d_model)\n",
    "        \n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...] # ...과 :의 차이점은?\n",
    "        \n",
    "        pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
    "        \n",
    "        return pos_encoding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        pos_encoding = self.positional_encoding(self.position, self.d_model)\n",
    "        \n",
    "        # tf.shape(inputs)[1]: 더하기위해 단순히 shape 맞추는 용도? 그 이상의 position encoding은 필요없음\n",
    "        return inputs + pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "still-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    ### einsum으로 변경 ('b h k d', 'b h q d' -> 'b h q k')\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \n",
    "        inputs = tf.reshape(inputs, (batch_size, -1, self.num_heads, self.depth))\n",
    "        \n",
    "        # transpose 하는 이유?: 곱셈을 위해!\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # The input consists of queries and keys of dimension dk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        \n",
    "        # We suspect that for large values of dk, the dot products grow large in magnitude,\n",
    "        # pushing the softmax function into regions where it has extremely small gradients. \n",
    "        # >> dot product가 커지는 것을 방지하기 위해 dk(lenght(depth)로 봄)가 커지면 grdient 훈련이 안되니까!!\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask.shape == (batch_size, 1, 1, seq_len_k)\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def call(self, q, k, v, mask):\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        wq = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(wq, batch_size)  # (batch_size, num_heads, seq_len, d_model/num_heads)\n",
    "        k = self.split_heads(wk, batch_size)\n",
    "        v = self.split_heads(wv, batch_size)\n",
    "        \n",
    "        # multi_head_attention.shape == (batch_size, num_heads, seq_len_q, depth_v)\n",
    "        # attention_weight.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        multi_head_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # transpose 하는 이유?: head split할 때 transpose함 -> head concat을 위해 shape 다시 돌려주기\n",
    "        multi_head_attention = tf.transpose(multi_head_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, d_model/num_heads)\n",
    "        \n",
    "        concat_attetion = tf.reshape(multi_head_attention,\n",
    "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        output = self.dense(concat_attetion)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-offense",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "strategic-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff,\n",
    "                         vocab_size, position, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # embedding layer + positional_encoding\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = PositionalEncoding(position, d_model)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate=rate)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # - position-wise FFNN ()\n",
    "        self.dense1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        # - attention (scaled-dot)\n",
    "        #     - self-attention\n",
    "        #     - multi-head\n",
    "        #     - padding masking\n",
    "        \n",
    "        # self.mha = MultiHeadAttention(d_model, num_heads) ## 한번만 부름 ALBERT\n",
    "        self.mha = [MultiHeadAttention(d_model, num_heads) \n",
    "                        for _ in range(num_layers)]\n",
    "    \n",
    "    def encoder_layer(self, inputs, training, mask, idx=None):\n",
    "        \n",
    "        # attention, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        attention, _ = self.mha[idx](inputs, inputs, inputs, mask)\n",
    "        \n",
    "        # - layer norm (& residual)\n",
    "        attention = self.dropout2(attention, training=training)\n",
    "        attention = self.layernorm1(inputs + attention)\n",
    "        \n",
    "        ffn = self.dense1(attention)\n",
    "        ffn = self.dense2(ffn)\n",
    "        \n",
    "        output = self.dropout3(ffn, training=training)\n",
    "        output = self.layernorm2(attention + output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def call(self, inputs, training, mask, token_type_id=None):\n",
    "        \n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs = self.pos_encoding(inputs)\n",
    "        \n",
    "#         print(inputs.shape, token_type_id.shape)\n",
    "        # euni\n",
    "        if token_type_id is not None:\n",
    "            inputs = tf.math.add(inputs, tf.expand_dims(token_type_id,-1)) \n",
    "        output = self.dropout1(inputs, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            output = self.encoder_layer(output, training, mask, idx=i)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-miami",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "native-summary",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "elegant-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff,\n",
    "                       vocab_size, position, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # - embedding layer + positional_encoding\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = PositionalEncoding(position, d_model)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate=rate)\n",
    "        self.dropout4 = tf.keras.layers.Dropout(rate=rate)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # - position-wise FFNN\n",
    "        self.dense1 = tf.keras.layers.Dense(self.d_ff, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        # - attention (scaled-dot)\n",
    "        #     - self-attention\n",
    "        #     - encoder-decoder attention\n",
    "        #     - multi-head\n",
    "        #     - mask\n",
    "        #         1. padding masking\n",
    "        #         2. 다음 token masking\n",
    "        # self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        # self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha1 = [MultiHeadAttention(d_model, num_heads) \n",
    "                        for _ in range(num_layers)]\n",
    "        self.mha2 = [MultiHeadAttention(d_model, num_heads) \n",
    "                        for _ in range(num_layers)]\n",
    "    \n",
    "    def decoder_layer(self, inputs, enc_output, training, look_ahead_mask, padding_mask, idx=None):\n",
    "        \n",
    "        ## attention을 왜 2개로 따로 하는지? - 첫 번째 masked input 자체를 두 번째 attention에 직접 넣지 않을까?\n",
    "        ## -> attention을 한번 더 통과하는 이유?\n",
    "        # self_attention, attention_weight1 = self.mha1(inputs, inputs, inputs, look_ahead_mask)\n",
    "        self_attention, attention_weight1 = self.mha1[idx](inputs, inputs, inputs, look_ahead_mask)\n",
    "        # - layer norm (& residual)\n",
    "        self_attention = self.dropout2(self_attention, training=training)\n",
    "        self_attention = self.layernorm1(inputs + self_attention)\n",
    "        \n",
    "        # q: decoder/ k=v: encoder\n",
    "        # enc_dec_attention, attention_weight2 = self.mha2(self_attention, enc_output, enc_output, padding_mask)\n",
    "        enc_dec_attention, attention_weight2 = self.mha2[idx](self_attention, enc_output, enc_output, padding_mask)\n",
    "        \n",
    "        enc_dec_attention = self.dropout3(enc_dec_attention, training=training)\n",
    "        enc_dec_attention = self.layernorm2(self_attention + enc_dec_attention)\n",
    "        \n",
    "        ffn = self.dense1(enc_dec_attention)\n",
    "        ffn = self.dense2(ffn)\n",
    "        \n",
    "        output = self.dropout4(ffn, training=training)\n",
    "        output = self.layernorm3(enc_dec_attention + output)\n",
    "        \n",
    "        return output, attention_weight1, attention_weight2\n",
    "    \n",
    "    def call(self, inputs, enc_output, training, look_ahead_mask, padding_mask, token_type_id=None):\n",
    "        \n",
    "        attention_weights = {}\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs = self.pos_encoding(inputs)\n",
    "\n",
    "        # euni\n",
    "        if token_type_id is not None:\n",
    "            inputs = tf.math.add(inputs, tf.expand_dims(token_type_id,-1))\n",
    "        \n",
    "        output = self.dropout1(inputs, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            output, block1, block2 = self.decoder_layer(output, enc_output, training, \n",
    "                                                            look_ahead_mask, padding_mask, idx=i)\n",
    "            \n",
    "            attention_weights[f'decoder_layer_{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer_{i+1}_block2'] = block2\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-substitute",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "nearby-expansion",
   "metadata": {},
   "source": [
    "## ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "suitable-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mask 생성하기\n",
    "\n",
    "# # padding_mask\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)  # 0인 부분을 1로 만듬(나중에 작은 음수값을 곱하기위해)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "# # look_ahead_mask\n",
    "def create_look_ahead_mask(x):\n",
    "    size = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)  #  Upper triangular part. (현재 값 이후의 부분 1로 만듬 -> 이유는 padding mask와 같음)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hearing-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210624, euni: random mask 추가\n",
    "def create_random_mask(x):\n",
    "    size = tf.shape(x)[1]\n",
    "    random_mask = np.zeros((size, size))\n",
    "    # 너무 많이 마스킹하지 않기위해\n",
    "    random_mask[np.random.rand(*random_mask.shape) < 0.15] = 1\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(random_mask, padding_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "consolidated-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 6), dtype=float32, numpy=array([[[[0., 0., 0., 0., 0., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_padding_mask(np.array([[1,2,2,3,4,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "miniature-direction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 6, 6), dtype=float32, numpy=\n",
       "array([[[[0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_look_ahead_mask(np.array([[1,2,2,3,4,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "romantic-germany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 6, 6), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_random_mask(np.array([[1,2,2,3,4,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-rendering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naked-institute",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "scheduled-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### main model\n",
    "# class Transformer(tf.keras.Model):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, d_ff, \n",
    "#                  input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "#         # d_model: dimension of model, d_ff: dimension of feed_forward\n",
    "#         # pe: positional_encoding\n",
    "#         super(Transformer, self).__init__()\n",
    "        \n",
    "#         ## encoder input, decoder에는 output: decoder의 Input과 Output은 같음\n",
    "#         ## -> 그래서 decoder에서 다음 token masking을 사용하는 것\n",
    "        \n",
    "#         # 인코더의 입력\n",
    "#         self.encoder = Encoder(num_layers, d_model, num_heads, d_ff,\n",
    "#                                  input_vocab_size, pe_input, rate)\n",
    "        \n",
    "#         # 디코더의 입력\n",
    "#         self.decoder = Decoder(num_layers, d_model, num_heads, d_ff,\n",
    "#                                target_vocab_size, pe_target, rate)\n",
    "        \n",
    "#         # 다음 단어 예측을 위한 출력층\n",
    "#         self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax', name='outputs')\n",
    "\n",
    "#     def call(self, inputs, targets, trainable=False, \n",
    "#              enc_padding_mask=None, dec_padding_mask=None,\n",
    "#              enc_token_type_id=None, dec_token_type_id=None):  # euni\n",
    "        \n",
    "#         if enc_padding_mask is None:\n",
    "#             enc_padding_mask = create_padding_mask(inputs)\n",
    "#         if dec_padding_mask is None:\n",
    "#             # 왜 input shape..? k, v를 encoder output으로 쓰는거랑 관련 있음 => mask.shape == (batch_size, 1, 1, seq_len_k)\n",
    "#             dec_padding_mask = create_padding_mask(inputs)\n",
    "        \n",
    "#         look_ahead_mask = create_look_ahead_mask(targets)\n",
    "        \n",
    "#         # 210624, euni: random mask 추가\n",
    "#         enc_random_mask = create_random_mask(inputs)\n",
    "#         enc_padding_mask = tf.maximum(enc_padding_mask, enc_random_mask)\n",
    "#         target_random_mask = create_random_mask(targets)\n",
    "#         look_ahead_mask = tf.maximum(look_ahead_mask, target_random_mask)\n",
    "\n",
    "#         enc_output = self.encoder(inputs, trainable, enc_padding_mask, enc_token_type_id)  # (batch_size, inp_seq_len, d_model)\n",
    "# #         print(inputs.shape,enc_output.shape, targets.shape)\n",
    "#         # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "#         dec_output, attention_weights = self.decoder(\n",
    "#                                             targets, enc_output, trainable, look_ahead_mask, dec_padding_mask, dec_token_type_id)\n",
    "\n",
    "#         final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "#         return final_output # , attention_weights  # attention weights를 output으로 가져오는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "surrounded-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "### main model\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, \n",
    "                 input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        # d_model: dimension of model, d_ff: dimension of feed_forward\n",
    "        # pe: positional_encoding\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        ## encoder input, decoder에는 output: decoder의 Input과 Output은 같음\n",
    "        ## -> 그래서 decoder에서 다음 token masking을 사용하는 것\n",
    "        \n",
    "        # 인코더의 입력\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff,\n",
    "                                 input_vocab_size, pe_input, rate)\n",
    "        \n",
    "        # 디코더의 입력\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "        \n",
    "        # 다음 단어 예측을 위한 출력층\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax', name='outputs')\n",
    "\n",
    "    def call(self, inputs, targets, trainable=False, \n",
    "             enc_padding_mask=None, dec_padding_mask=None,\n",
    "             enc_token_type_id=None, dec_token_type_id=None):  # euni\n",
    "        \n",
    "        if enc_padding_mask is None:\n",
    "            enc_padding_mask = create_padding_mask(inputs)\n",
    "        if dec_padding_mask is None:\n",
    "            # 왜 input shape..? k, v를 encoder output으로 쓰는거랑 관련 있음 => mask.shape == (batch_size, 1, 1, seq_len_k)\n",
    "            dec_padding_mask = create_padding_mask(inputs)\n",
    "        \n",
    "        look_ahead_mask = create_look_ahead_mask(targets)\n",
    "        \n",
    "#         # 210624, euni: random mask 추가\n",
    "#         enc_random_mask = create_random_mask(inputs)\n",
    "#         enc_padding_mask = tf.maximum(enc_padding_mask, enc_random_mask)\n",
    "#         target_random_mask = create_random_mask(targets)\n",
    "#         look_ahead_mask = tf.maximum(look_ahead_mask, target_random_mask)\n",
    "\n",
    "        enc_output = self.encoder(inputs, trainable, enc_padding_mask, enc_token_type_id)  # (batch_size, inp_seq_len, d_model)\n",
    "#         print(inputs.shape,enc_output.shape, targets.shape)\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "                                            targets, enc_output, trainable, look_ahead_mask, dec_padding_mask, dec_token_type_id)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output # , attention_weights  # attention weights를 output으로 가져오는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "entertaining-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample test\n",
    "# sample_transformer = Transformer(\n",
    "#     num_layers=2, d_model=512, num_heads=8, d_ff=2048,\n",
    "#     input_vocab_size=8500, target_vocab_size=8000,\n",
    "#     pe_input=10000, pe_target=6000)\n",
    "\n",
    "# temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "# temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "# fn_out = sample_transformer(temp_input, temp_target, trainable=False)\n",
    "\n",
    "# fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "imported-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-witness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-geography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "enabling-tiffany",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "hawaiian-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample test\n",
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=64, num_heads=8, d_ff=128,\n",
    "    input_vocab_size=tokenizer.vocab_size, target_vocab_size=tokenizer.vocab_size,\n",
    "    pe_input=128, pe_target=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "optical-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss\n",
    "def loss_function(real, pred):\n",
    "    # 타겟 시퀀스가 패딩되기 때문에 손실을 계산할 때 패딩 마스크를 적용하는 것이 중요합니다.(from google)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "included-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## acc (from google), 모르겠다 \n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(tar[:,0], tf.cast(tf.argmax(pred, axis=2), dtype=tf.float32))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "flying-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer (from google)\n",
    "# learning_rate = d_model**(-0.5)*min(step_num**(-0.5), step_num*warmup_steps**(-1.5)) # from 논문\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        ## warmup step 마다 다시 optimizer lr이 처음으로 돌아가는거?\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "skilled-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model=64)\n",
    "\n",
    "## optimizer: Adam( β1 = 0.9, β2 = 0.98 and epsilon= 10−9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-warehouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-german",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "daily-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train (with teacher forcing) -> type token 추가하는 layer 변경 필요, end token 있는데 padding 해줘야할까..?\n",
    "#  (짧은 seq 9:1 긴 seq -> 마지막에 추가훈련처럼 ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "small-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (구조변경-sigmoid 제거) lr: lr_schedule(0.005)\n",
    "# Epoch 20 MAE 0.1612 MSE 0.0769\n",
    "checkpoint_path = \"./checkpoints/train0\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(sample_transformer=sample_transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "editorial-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "virgin-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "copyrighted-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "# train_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, 3, 512), dtype=tf.int64),\n",
    "#     tf.TensorSpec(shape=(None, 3, 128), dtype=tf.int64),\n",
    "# ]\n",
    "\n",
    "\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar, training=True):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # batch\n",
    "        # {'input_ids': [101, 4931, 102, 3828, 102], 'token_type_ids': [0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
    "\n",
    "        predictions = sample_transformer(inp[:,0], tar[:,0],\n",
    "                                        enc_padding_mask=create_padding_mask(inp[:,-1]),\n",
    "                                        dec_padding_mask=create_padding_mask(inp[:,-1]),\n",
    "                                        enc_token_type_id=inp[:,1], dec_token_type_id=tar[:,1])\n",
    "        loss = loss_function(tar[:,0], predictions)\n",
    "    gradients = tape.gradient(loss, sample_transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sample_transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar[:,0], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "linear-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "# val_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, 3, 512), dtype=tf.int64),\n",
    "#     tf.TensorSpec(shape=(None, 3, 128), dtype=tf.int64),\n",
    "# ]\n",
    "\n",
    "\n",
    "# @tf.function(input_signature=val_step_signature)\n",
    "def val_step(inp, tar, training=False):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = sample_transformer(inp[:,0], tar[:,0],\n",
    "                            enc_padding_mask=create_padding_mask(inp[:,-1]),\n",
    "                            dec_padding_mask=create_padding_mask(inp[:,-1]),\n",
    "                            enc_token_type_id=inp[:,1], dec_token_type_id=tar[:,1])\n",
    "        loss = loss_function(tar[:,0], predictions)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(accuracy_function(tar[:,0], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fuzzy-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bulgarian-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, tar in train_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ideal-sphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 3, 128]), TensorShape([16, 3, 32]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape, tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "saving-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sample_transformer(inp[:,0], tar[:,0],\n",
    "                    enc_padding_mask=create_padding_mask(inp[:,-1]),\n",
    "                    dec_padding_mask=create_padding_mask(inp[:,-1]),\n",
    "                    enc_token_type_id=inp[:,1], dec_token_type_id=tar[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "indonesian-palestinian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 3, 128]), TensorShape([16, 3, 32]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape, tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "weekly-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 100 Loss 10.2378 Accuracy 0.0528\n",
      "Epoch 1 Batch 200 Loss 10.0434 Accuracy 0.1171\n",
      "Epoch 1 Batch 300 Loss 9.7553 Accuracy 0.1344\n",
      "Epoch 1 Batch 400 Loss 9.3891 Accuracy 0.1429\n",
      "Epoch 1 Batch 500 Loss 8.9451 Accuracy 0.1549\n",
      "Epoch 1 Batch 600 Loss 8.5271 Accuracy 0.1781\n",
      "Epoch 1 Batch 700 Loss 8.1419 Accuracy 0.2044\n",
      "Epoch 1 Batch 800 Loss 7.8058 Accuracy 0.2217\n",
      "Epoch 1 Batch 900 Loss 7.5159 Accuracy 0.2350\n",
      "Epoch 1 Batch 1000 Loss 7.2667 Accuracy 0.2491\n",
      "Epoch 1 Batch 1100 Loss 7.0486 Accuracy 0.2633\n",
      "Epoch 1 Batch 1200 Loss 6.8472 Accuracy 0.2769\n",
      "Epoch 1 Batch 1300 Loss 6.6468 Accuracy 0.2919\n",
      "Epoch 1 Batch 1400 Loss 6.4655 Accuracy 0.3050\n",
      "Epoch 1 Batch 1500 Loss 6.3078 Accuracy 0.3162\n",
      "Epoch 1 Batch 1600 Loss 6.1573 Accuracy 0.3276\n",
      "Epoch 1 Batch 1700 Loss 6.0156 Accuracy 0.3385\n",
      "Epoch 1 Batch 1800 Loss 5.8709 Accuracy 0.3505\n",
      "Epoch 1 Batch 1900 Loss 5.7309 Accuracy 0.3629\n",
      "Epoch 1 Batch 2000 Loss 5.5992 Accuracy 0.3747\n",
      "Epoch 1 Batch 2100 Loss 5.4723 Accuracy 0.3864\n",
      "Epoch 1 Batch 2200 Loss 5.3532 Accuracy 0.3976\n",
      "Epoch 1 Batch 2300 Loss 5.2318 Accuracy 0.4098\n",
      "Epoch 1 Batch 2400 Loss 5.1111 Accuracy 0.4220\n",
      "Epoch 1 Batch 2500 Loss 4.9979 Accuracy 0.4336\n",
      "Epoch 1 Batch 2600 Loss 4.8924 Accuracy 0.4445\n",
      "Epoch 1 Batch 2700 Loss 4.7883 Accuracy 0.4556\n",
      "Epoch 1 Batch 2800 Loss 4.6878 Accuracy 0.4663\n",
      "Epoch 1 Batch 2900 Loss 4.5895 Accuracy 0.4770\n",
      "Epoch 1 Batch 3000 Loss 4.4936 Accuracy 0.4874\n",
      "Epoch 1 Batch 3100 Loss 4.4048 Accuracy 0.4971\n",
      "Epoch 1 Batch 3200 Loss 4.3186 Accuracy 0.5066\n",
      "Epoch 1 Batch 3300 Loss 4.2365 Accuracy 0.5159\n",
      "Epoch 1 Batch 3400 Loss 4.1574 Accuracy 0.5248\n",
      "Epoch 1 Batch 3500 Loss 4.0801 Accuracy 0.5334\n",
      "Epoch 1 Batch 3600 Loss 4.0064 Accuracy 0.5418\n",
      "Epoch 1 Batch 3700 Loss 3.9373 Accuracy 0.5497\n",
      "Epoch 1 Batch 3800 Loss 3.8725 Accuracy 0.5571\n",
      "Epoch 1 Batch 3900 Loss 3.8109 Accuracy 0.5641\n",
      "Epoch 1 Batch 4000 Loss 3.7503 Accuracy 0.5709\n",
      "Epoch 1 Batch 4100 Loss 3.6927 Accuracy 0.5776\n",
      "Epoch 1 Batch 4200 Loss 3.6392 Accuracy 0.5837\n",
      "Epoch 1 Batch 4300 Loss 3.5860 Accuracy 0.5898\n",
      "Epoch 1 Batch 4400 Loss 3.5356 Accuracy 0.5956\n",
      "Epoch 1 Batch 4500 Loss 3.4870 Accuracy 0.6013\n",
      "Epoch 1 Batch 4600 Loss 3.4396 Accuracy 0.6067\n",
      "Epoch 1 Batch 4700 Loss 3.3948 Accuracy 0.6119\n",
      "Epoch 1 Batch 4800 Loss 3.3527 Accuracy 0.6168\n",
      "Epoch 1 Batch 4900 Loss 3.3117 Accuracy 0.6217\n",
      "Epoch 1 Batch 5000 Loss 3.2712 Accuracy 0.6264\n",
      "Epoch 1 Batch 5100 Loss 3.2331 Accuracy 0.6309\n",
      "Epoch 1 Batch 5200 Loss 3.1961 Accuracy 0.6352\n",
      "Epoch 1 Batch 5300 Loss 3.1616 Accuracy 0.6392\n",
      "Epoch 1 Batch 5400 Loss 3.1283 Accuracy 0.6431\n",
      "Epoch 1 Batch 5500 Loss 3.0958 Accuracy 0.6470\n",
      "Epoch 1 Batch 5600 Loss 3.0644 Accuracy 0.6506\n",
      "Epoch 1 Batch 5700 Loss 3.0337 Accuracy 0.6543\n",
      "Epoch 1 Batch 5800 Loss 3.0056 Accuracy 0.6576\n",
      "Epoch 1 Batch 5900 Loss 2.9777 Accuracy 0.6608\n",
      "Epoch 1 Batch 6000 Loss 2.9501 Accuracy 0.6641\n",
      "Epoch 1 Batch 6100 Loss 2.9237 Accuracy 0.6673\n",
      "Epoch 1 Batch 6200 Loss 2.8978 Accuracy 0.6703\n",
      "Epoch 1 Batch 6300 Loss 2.8734 Accuracy 0.6732\n",
      "Epoch 1 Batch 6400 Loss 2.8498 Accuracy 0.6760\n",
      "Epoch 1 Batch 6500 Loss 2.8269 Accuracy 0.6788\n",
      "Epoch 1 Batch 6600 Loss 2.8053 Accuracy 0.6813\n",
      "Epoch 1 Batch 6700 Loss 2.7831 Accuracy 0.6840\n",
      "Epoch 1 Batch 6800 Loss 2.7614 Accuracy 0.6865\n",
      "Epoch 1 Batch 6900 Loss 2.7410 Accuracy 0.6889\n",
      "Epoch 1 Batch 7000 Loss 2.7214 Accuracy 0.6913\n",
      "Epoch 1 Batch 7100 Loss 2.7026 Accuracy 0.6935\n",
      "Epoch 1 Batch 7200 Loss 2.6836 Accuracy 0.6957\n",
      "Epoch 1 Batch 7300 Loss 2.6650 Accuracy 0.6979\n",
      "Epoch 1 Batch 7400 Loss 2.6478 Accuracy 0.6999\n",
      "Epoch 1 Batch 7500 Loss 2.6308 Accuracy 0.7019\n",
      "Epoch 1 Batch 7600 Loss 2.6135 Accuracy 0.7040\n",
      "Epoch 1 Batch 7700 Loss 2.5978 Accuracy 0.7058\n",
      "Epoch 1 Batch 7800 Loss 2.5822 Accuracy 0.7076\n",
      "Epoch 1 Batch 7900 Loss 2.5663 Accuracy 0.7095\n",
      "Epoch 1 Batch 8000 Loss 2.5516 Accuracy 0.7113\n",
      "Epoch 1 Batch 8100 Loss 2.5377 Accuracy 0.7129\n",
      "Epoch 1 Batch 8200 Loss 2.5234 Accuracy 0.7146\n",
      "Epoch 1 Batch 8300 Loss 2.5095 Accuracy 0.7162\n",
      "Epoch 1 Batch 8400 Loss 2.4956 Accuracy 0.7179\n",
      "Epoch 1 Batch 8500 Loss 2.4827 Accuracy 0.7194\n",
      "Epoch 1 Batch 8600 Loss 2.4700 Accuracy 0.7208\n",
      "Epoch 1 Loss 2.4584 Accuracy 0.7222\n",
      "Time taken for 1 epoch: 2251.90 secs\n",
      "\n",
      "Epoch 2 Batch 100 Loss 1.4691 Accuracy 0.8359\n",
      "Epoch 2 Batch 200 Loss 1.4035 Accuracy 0.8435\n",
      "Epoch 2 Batch 300 Loss 1.3927 Accuracy 0.8447\n",
      "Epoch 2 Batch 400 Loss 1.3912 Accuracy 0.8458\n",
      "Epoch 2 Batch 500 Loss 1.3907 Accuracy 0.8464\n",
      "Epoch 2 Batch 600 Loss 1.3864 Accuracy 0.8471\n",
      "Epoch 2 Batch 700 Loss 1.3885 Accuracy 0.8467\n",
      "Epoch 2 Batch 800 Loss 1.3848 Accuracy 0.8470\n",
      "Epoch 2 Batch 900 Loss 1.3843 Accuracy 0.8470\n",
      "Epoch 2 Batch 1000 Loss 1.3810 Accuracy 0.8474\n",
      "Epoch 2 Batch 1100 Loss 1.3813 Accuracy 0.8474\n",
      "Epoch 2 Batch 1200 Loss 1.3815 Accuracy 0.8475\n",
      "Epoch 2 Batch 1300 Loss 1.3811 Accuracy 0.8473\n",
      "Epoch 2 Batch 1400 Loss 1.3768 Accuracy 0.8477\n",
      "Epoch 2 Batch 1500 Loss 1.3778 Accuracy 0.8476\n",
      "Epoch 2 Batch 1600 Loss 1.3768 Accuracy 0.8477\n",
      "Epoch 2 Batch 1700 Loss 1.3783 Accuracy 0.8475\n",
      "Epoch 2 Batch 1800 Loss 1.3780 Accuracy 0.8475\n",
      "Epoch 2 Batch 1900 Loss 1.3765 Accuracy 0.8477\n",
      "Epoch 2 Batch 2000 Loss 1.3757 Accuracy 0.8477\n",
      "Epoch 2 Batch 2100 Loss 1.3743 Accuracy 0.8478\n",
      "Epoch 2 Batch 2200 Loss 1.3741 Accuracy 0.8479\n",
      "Epoch 2 Batch 2300 Loss 1.3733 Accuracy 0.8480\n",
      "Epoch 2 Batch 2400 Loss 1.3721 Accuracy 0.8481\n",
      "Epoch 2 Batch 2500 Loss 1.3733 Accuracy 0.8479\n",
      "Epoch 2 Batch 2600 Loss 1.3743 Accuracy 0.8479\n",
      "Epoch 2 Batch 2700 Loss 1.3742 Accuracy 0.8479\n",
      "Epoch 2 Batch 2800 Loss 1.3736 Accuracy 0.8480\n",
      "Epoch 2 Batch 2900 Loss 1.3723 Accuracy 0.8480\n",
      "Epoch 2 Batch 3000 Loss 1.3706 Accuracy 0.8481\n",
      "Epoch 2 Batch 3100 Loss 1.3725 Accuracy 0.8479\n",
      "Epoch 2 Batch 3200 Loss 1.3707 Accuracy 0.8481\n",
      "Epoch 2 Batch 3300 Loss 1.3699 Accuracy 0.8482\n",
      "Epoch 2 Batch 3400 Loss 1.3711 Accuracy 0.8479\n",
      "Epoch 2 Batch 3500 Loss 1.3705 Accuracy 0.8479\n",
      "Epoch 2 Batch 3600 Loss 1.3706 Accuracy 0.8478\n",
      "Epoch 2 Batch 3700 Loss 1.3709 Accuracy 0.8477\n",
      "Epoch 2 Batch 3800 Loss 1.3713 Accuracy 0.8477\n",
      "Epoch 2 Batch 3900 Loss 1.3714 Accuracy 0.8476\n",
      "Epoch 2 Batch 4000 Loss 1.3710 Accuracy 0.8475\n",
      "Epoch 2 Batch 4100 Loss 1.3709 Accuracy 0.8474\n",
      "Epoch 2 Batch 4200 Loss 1.3716 Accuracy 0.8472\n",
      "Epoch 2 Batch 4300 Loss 1.3707 Accuracy 0.8473\n",
      "Epoch 2 Batch 4400 Loss 1.3706 Accuracy 0.8473\n",
      "Epoch 2 Batch 4500 Loss 1.3708 Accuracy 0.8472\n",
      "Epoch 2 Batch 4600 Loss 1.3695 Accuracy 0.8472\n",
      "Epoch 2 Batch 4700 Loss 1.3702 Accuracy 0.8471\n",
      "Epoch 2 Batch 4800 Loss 1.3702 Accuracy 0.8471\n",
      "Epoch 2 Batch 4900 Loss 1.3698 Accuracy 0.8471\n",
      "Epoch 2 Batch 5000 Loss 1.3692 Accuracy 0.8471\n",
      "Epoch 2 Batch 5100 Loss 1.3685 Accuracy 0.8471\n",
      "Epoch 2 Batch 5200 Loss 1.3689 Accuracy 0.8470\n",
      "Epoch 2 Batch 5300 Loss 1.3700 Accuracy 0.8468\n",
      "Epoch 2 Batch 5400 Loss 1.3703 Accuracy 0.8467\n",
      "Epoch 2 Batch 5500 Loss 1.3704 Accuracy 0.8467\n",
      "Epoch 2 Batch 5600 Loss 1.3693 Accuracy 0.8467\n",
      "Epoch 2 Batch 5700 Loss 1.3689 Accuracy 0.8467\n",
      "Epoch 2 Batch 5800 Loss 1.3691 Accuracy 0.8466\n",
      "Epoch 2 Batch 5900 Loss 1.3707 Accuracy 0.8463\n",
      "Epoch 2 Batch 6000 Loss 1.3694 Accuracy 0.8465\n",
      "Epoch 2 Batch 6100 Loss 1.3695 Accuracy 0.8464\n",
      "Epoch 2 Batch 6200 Loss 1.3694 Accuracy 0.8463\n",
      "Epoch 2 Batch 6300 Loss 1.3690 Accuracy 0.8463\n",
      "Epoch 2 Batch 6400 Loss 1.3688 Accuracy 0.8464\n",
      "Epoch 2 Batch 6500 Loss 1.3689 Accuracy 0.8463\n",
      "Epoch 2 Batch 6600 Loss 1.3683 Accuracy 0.8463\n",
      "Epoch 2 Batch 6700 Loss 1.3686 Accuracy 0.8462\n",
      "Epoch 2 Batch 6800 Loss 1.3688 Accuracy 0.8461\n",
      "Epoch 2 Batch 6900 Loss 1.3691 Accuracy 0.8460\n",
      "Epoch 2 Batch 7000 Loss 1.3690 Accuracy 0.8460\n",
      "Epoch 2 Batch 7100 Loss 1.3688 Accuracy 0.8460\n",
      "Epoch 2 Batch 7200 Loss 1.3689 Accuracy 0.8459\n",
      "Epoch 2 Batch 7300 Loss 1.3679 Accuracy 0.8459\n",
      "Epoch 2 Batch 7400 Loss 1.3677 Accuracy 0.8459\n",
      "Epoch 2 Batch 7500 Loss 1.3670 Accuracy 0.8460\n",
      "Epoch 2 Batch 7600 Loss 1.3672 Accuracy 0.8459\n",
      "Epoch 2 Batch 7700 Loss 1.3677 Accuracy 0.8458\n",
      "Epoch 2 Batch 7800 Loss 1.3672 Accuracy 0.8458\n",
      "Epoch 2 Batch 7900 Loss 1.3666 Accuracy 0.8458\n",
      "Epoch 2 Batch 8000 Loss 1.3672 Accuracy 0.8458\n",
      "Epoch 2 Batch 8100 Loss 1.3667 Accuracy 0.8458\n",
      "Epoch 2 Batch 8200 Loss 1.3667 Accuracy 0.8458\n",
      "Epoch 2 Batch 8300 Loss 1.3669 Accuracy 0.8457\n",
      "Epoch 2 Batch 8400 Loss 1.3662 Accuracy 0.8457\n",
      "Epoch 2 Batch 8500 Loss 1.3664 Accuracy 0.8457\n",
      "Epoch 2 Batch 8600 Loss 1.3659 Accuracy 0.8457\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-1\n",
      "val-dataset result: Loss 3.2341 Accuracy 0.7864\n",
      "Epoch 2 Loss 1.3651 Accuracy 0.8458\n",
      "Time taken for 1 epoch: 3389.97 secs\n",
      "\n",
      "Epoch 3 Batch 100 Loss 1.4782 Accuracy 0.8287\n",
      "Epoch 3 Batch 200 Loss 1.3873 Accuracy 0.8397\n",
      "Epoch 3 Batch 300 Loss 1.3710 Accuracy 0.8412\n",
      "Epoch 3 Batch 400 Loss 1.3766 Accuracy 0.8409\n",
      "Epoch 3 Batch 500 Loss 1.3768 Accuracy 0.8412\n",
      "Epoch 3 Batch 600 Loss 1.3780 Accuracy 0.8416\n",
      "Epoch 3 Batch 700 Loss 1.3738 Accuracy 0.8421\n",
      "Epoch 3 Batch 800 Loss 1.3640 Accuracy 0.8435\n",
      "Epoch 3 Batch 900 Loss 1.3616 Accuracy 0.8438\n",
      "Epoch 3 Batch 1000 Loss 1.3624 Accuracy 0.8438\n",
      "Epoch 3 Batch 1100 Loss 1.3617 Accuracy 0.8438\n",
      "Epoch 3 Batch 1200 Loss 1.3641 Accuracy 0.8436\n",
      "Epoch 3 Batch 1300 Loss 1.3581 Accuracy 0.8440\n",
      "Epoch 3 Batch 1400 Loss 1.3568 Accuracy 0.8440\n",
      "Epoch 3 Batch 1500 Loss 1.3574 Accuracy 0.8440\n",
      "Epoch 3 Batch 1600 Loss 1.3564 Accuracy 0.8442\n",
      "Epoch 3 Batch 1700 Loss 1.3556 Accuracy 0.8444\n",
      "Epoch 3 Batch 1800 Loss 1.3540 Accuracy 0.8446\n",
      "Epoch 3 Batch 1900 Loss 1.3525 Accuracy 0.8448\n",
      "Epoch 3 Batch 2000 Loss 1.3554 Accuracy 0.8446\n",
      "Epoch 3 Batch 2100 Loss 1.3547 Accuracy 0.8448\n",
      "Epoch 3 Batch 2200 Loss 1.3536 Accuracy 0.8449\n",
      "Epoch 3 Batch 2300 Loss 1.3531 Accuracy 0.8449\n",
      "Epoch 3 Batch 2400 Loss 1.3502 Accuracy 0.8452\n",
      "Epoch 3 Batch 2500 Loss 1.3498 Accuracy 0.8452\n",
      "Epoch 3 Batch 2600 Loss 1.3493 Accuracy 0.8453\n",
      "Epoch 3 Batch 2700 Loss 1.3479 Accuracy 0.8455\n",
      "Epoch 3 Batch 2800 Loss 1.3481 Accuracy 0.8454\n",
      "Epoch 3 Batch 2900 Loss 1.3480 Accuracy 0.8455\n",
      "Epoch 3 Batch 3000 Loss 1.3475 Accuracy 0.8456\n",
      "Epoch 3 Batch 3100 Loss 1.3487 Accuracy 0.8455\n",
      "Epoch 3 Batch 3200 Loss 1.3476 Accuracy 0.8458\n",
      "Epoch 3 Batch 3300 Loss 1.3470 Accuracy 0.8459\n",
      "Epoch 3 Batch 3400 Loss 1.3482 Accuracy 0.8457\n",
      "Epoch 3 Batch 3500 Loss 1.3468 Accuracy 0.8458\n",
      "Epoch 3 Batch 3600 Loss 1.3469 Accuracy 0.8458\n",
      "Epoch 3 Batch 3700 Loss 1.3474 Accuracy 0.8457\n",
      "Epoch 3 Batch 3800 Loss 1.3469 Accuracy 0.8458\n",
      "Epoch 3 Batch 3900 Loss 1.3471 Accuracy 0.8458\n",
      "Epoch 3 Batch 4000 Loss 1.3471 Accuracy 0.8457\n",
      "Epoch 3 Batch 4100 Loss 1.3463 Accuracy 0.8459\n",
      "Epoch 3 Batch 4200 Loss 1.3467 Accuracy 0.8458\n",
      "Epoch 3 Batch 4300 Loss 1.3463 Accuracy 0.8459\n",
      "Epoch 3 Batch 4400 Loss 1.3468 Accuracy 0.8459\n",
      "Epoch 3 Batch 4500 Loss 1.3462 Accuracy 0.8459\n",
      "Epoch 3 Batch 4600 Loss 1.3451 Accuracy 0.8460\n",
      "Epoch 3 Batch 4700 Loss 1.3452 Accuracy 0.8460\n",
      "Epoch 3 Batch 4800 Loss 1.3451 Accuracy 0.8461\n",
      "Epoch 3 Batch 4900 Loss 1.3446 Accuracy 0.8462\n",
      "Epoch 3 Batch 5000 Loss 1.3448 Accuracy 0.8461\n",
      "Epoch 3 Batch 5100 Loss 1.3448 Accuracy 0.8461\n",
      "Epoch 3 Batch 5200 Loss 1.3451 Accuracy 0.8461\n",
      "Epoch 3 Batch 5300 Loss 1.3460 Accuracy 0.8460\n",
      "Epoch 3 Batch 5400 Loss 1.3451 Accuracy 0.8462\n",
      "Epoch 3 Batch 5500 Loss 1.3449 Accuracy 0.8462\n",
      "Epoch 3 Batch 5600 Loss 1.3455 Accuracy 0.8462\n",
      "Epoch 3 Batch 5700 Loss 1.3450 Accuracy 0.8462\n",
      "Epoch 3 Batch 5800 Loss 1.3458 Accuracy 0.8461\n",
      "Epoch 3 Batch 5900 Loss 1.3450 Accuracy 0.8462\n",
      "Epoch 3 Batch 6000 Loss 1.3457 Accuracy 0.8462\n",
      "Epoch 3 Batch 6100 Loss 1.3451 Accuracy 0.8463\n",
      "Epoch 3 Batch 6200 Loss 1.3436 Accuracy 0.8464\n",
      "Epoch 3 Batch 6300 Loss 1.3438 Accuracy 0.8464\n",
      "Epoch 3 Batch 6400 Loss 1.3437 Accuracy 0.8464\n",
      "Epoch 3 Batch 6500 Loss 1.3434 Accuracy 0.8465\n",
      "Epoch 3 Batch 6600 Loss 1.3439 Accuracy 0.8465\n",
      "Epoch 3 Batch 6700 Loss 1.3434 Accuracy 0.8465\n",
      "Epoch 3 Batch 6800 Loss 1.3428 Accuracy 0.8466\n",
      "Epoch 3 Batch 6900 Loss 1.3435 Accuracy 0.8466\n",
      "Epoch 3 Batch 7000 Loss 1.3433 Accuracy 0.8466\n",
      "Epoch 3 Batch 7100 Loss 1.3426 Accuracy 0.8467\n",
      "Epoch 3 Batch 7200 Loss 1.3425 Accuracy 0.8467\n",
      "Epoch 3 Batch 7300 Loss 1.3419 Accuracy 0.8468\n",
      "Epoch 3 Batch 7400 Loss 1.3421 Accuracy 0.8468\n",
      "Epoch 3 Batch 7500 Loss 1.3425 Accuracy 0.8467\n",
      "Epoch 3 Batch 7600 Loss 1.3422 Accuracy 0.8468\n",
      "Epoch 3 Batch 7700 Loss 1.3426 Accuracy 0.8468\n",
      "Epoch 3 Batch 7800 Loss 1.3421 Accuracy 0.8468\n",
      "Epoch 3 Batch 7900 Loss 1.3417 Accuracy 0.8469\n",
      "Epoch 3 Batch 8000 Loss 1.3416 Accuracy 0.8469\n",
      "Epoch 3 Batch 8100 Loss 1.3421 Accuracy 0.8469\n",
      "Epoch 3 Batch 8200 Loss 1.3420 Accuracy 0.8469\n",
      "Epoch 3 Batch 8300 Loss 1.3421 Accuracy 0.8469\n",
      "Epoch 3 Batch 8400 Loss 1.3416 Accuracy 0.8470\n",
      "Epoch 3 Batch 8500 Loss 1.3416 Accuracy 0.8470\n",
      "Epoch 3 Batch 8600 Loss 1.3413 Accuracy 0.8471\n",
      "Epoch 3 Loss 1.3405 Accuracy 0.8471\n",
      "Time taken for 1 epoch: 2258.97 secs\n",
      "\n",
      "Epoch 4 Batch 100 Loss 1.4311 Accuracy 0.8363\n",
      "Epoch 4 Batch 200 Loss 1.3605 Accuracy 0.8431\n",
      "Epoch 4 Batch 300 Loss 1.3315 Accuracy 0.8472\n",
      "Epoch 4 Batch 400 Loss 1.3427 Accuracy 0.8468\n",
      "Epoch 4 Batch 500 Loss 1.3418 Accuracy 0.8474\n",
      "Epoch 4 Batch 600 Loss 1.3361 Accuracy 0.8488\n",
      "Epoch 4 Batch 700 Loss 1.3393 Accuracy 0.8482\n",
      "Epoch 4 Batch 800 Loss 1.3373 Accuracy 0.8483\n",
      "Epoch 4 Batch 900 Loss 1.3399 Accuracy 0.8479\n",
      "Epoch 4 Batch 1000 Loss 1.3367 Accuracy 0.8484\n",
      "Epoch 4 Batch 1100 Loss 1.3346 Accuracy 0.8488\n",
      "Epoch 4 Batch 1200 Loss 1.3361 Accuracy 0.8487\n",
      "Epoch 4 Batch 1300 Loss 1.3343 Accuracy 0.8488\n",
      "Epoch 4 Batch 1400 Loss 1.3342 Accuracy 0.8488\n",
      "Epoch 4 Batch 1500 Loss 1.3350 Accuracy 0.8488\n",
      "Epoch 4 Batch 1600 Loss 1.3305 Accuracy 0.8495\n",
      "Epoch 4 Batch 1700 Loss 1.3323 Accuracy 0.8495\n",
      "Epoch 4 Batch 1800 Loss 1.3323 Accuracy 0.8492\n",
      "Epoch 4 Batch 1900 Loss 1.3299 Accuracy 0.8495\n",
      "Epoch 4 Batch 2000 Loss 1.3328 Accuracy 0.8493\n",
      "Epoch 4 Batch 2100 Loss 1.3336 Accuracy 0.8491\n",
      "Epoch 4 Batch 2200 Loss 1.3326 Accuracy 0.8495\n",
      "Epoch 4 Batch 2300 Loss 1.3321 Accuracy 0.8496\n",
      "Epoch 4 Batch 2400 Loss 1.3303 Accuracy 0.8498\n",
      "Epoch 4 Batch 2500 Loss 1.3302 Accuracy 0.8498\n",
      "Epoch 4 Batch 2600 Loss 1.3311 Accuracy 0.8498\n",
      "Epoch 4 Batch 2700 Loss 1.3307 Accuracy 0.8499\n",
      "Epoch 4 Batch 2800 Loss 1.3314 Accuracy 0.8498\n",
      "Epoch 4 Batch 2900 Loss 1.3312 Accuracy 0.8499\n",
      "Epoch 4 Batch 3000 Loss 1.3297 Accuracy 0.8501\n",
      "Epoch 4 Batch 3100 Loss 1.3305 Accuracy 0.8501\n",
      "Epoch 4 Batch 3200 Loss 1.3316 Accuracy 0.8499\n",
      "Epoch 4 Batch 3300 Loss 1.3318 Accuracy 0.8500\n",
      "Epoch 4 Batch 3400 Loss 1.3324 Accuracy 0.8499\n",
      "Epoch 4 Batch 3500 Loss 1.3318 Accuracy 0.8500\n",
      "Epoch 4 Batch 3600 Loss 1.3305 Accuracy 0.8501\n",
      "Epoch 4 Batch 3700 Loss 1.3304 Accuracy 0.8502\n",
      "Epoch 4 Batch 3800 Loss 1.3309 Accuracy 0.8501\n",
      "Epoch 4 Batch 3900 Loss 1.3311 Accuracy 0.8501\n",
      "Epoch 4 Batch 4000 Loss 1.3300 Accuracy 0.8502\n",
      "Epoch 4 Batch 4100 Loss 1.3288 Accuracy 0.8503\n",
      "Epoch 4 Batch 4200 Loss 1.3300 Accuracy 0.8502\n",
      "Epoch 4 Batch 4300 Loss 1.3303 Accuracy 0.8502\n",
      "Epoch 4 Batch 4400 Loss 1.3309 Accuracy 0.8502\n",
      "Epoch 4 Batch 4500 Loss 1.3314 Accuracy 0.8501\n",
      "Epoch 4 Batch 4600 Loss 1.3307 Accuracy 0.8502\n",
      "Epoch 4 Batch 4700 Loss 1.3302 Accuracy 0.8503\n",
      "Epoch 4 Batch 4800 Loss 1.3297 Accuracy 0.8503\n",
      "Epoch 4 Batch 4900 Loss 1.3301 Accuracy 0.8503\n",
      "Epoch 4 Batch 5000 Loss 1.3301 Accuracy 0.8503\n",
      "Epoch 4 Batch 5100 Loss 1.3294 Accuracy 0.8504\n",
      "Epoch 4 Batch 5200 Loss 1.3288 Accuracy 0.8505\n",
      "Epoch 4 Batch 5300 Loss 1.3282 Accuracy 0.8506\n",
      "Epoch 4 Batch 5400 Loss 1.3279 Accuracy 0.8507\n",
      "Epoch 4 Batch 5500 Loss 1.3275 Accuracy 0.8507\n",
      "Epoch 4 Batch 5600 Loss 1.3273 Accuracy 0.8508\n",
      "Epoch 4 Batch 5700 Loss 1.3275 Accuracy 0.8508\n",
      "Epoch 4 Batch 5800 Loss 1.3276 Accuracy 0.8507\n",
      "Epoch 4 Batch 5900 Loss 1.3269 Accuracy 0.8509\n",
      "Epoch 4 Batch 6000 Loss 1.3265 Accuracy 0.8509\n",
      "Epoch 4 Batch 6100 Loss 1.3270 Accuracy 0.8509\n",
      "Epoch 4 Batch 6200 Loss 1.3266 Accuracy 0.8509\n",
      "Epoch 4 Batch 6300 Loss 1.3268 Accuracy 0.8509\n",
      "Epoch 4 Batch 6400 Loss 1.3268 Accuracy 0.8509\n",
      "Epoch 4 Batch 6500 Loss 1.3266 Accuracy 0.8510\n",
      "Epoch 4 Batch 6600 Loss 1.3267 Accuracy 0.8510\n",
      "Epoch 4 Batch 6700 Loss 1.3262 Accuracy 0.8510\n",
      "Epoch 4 Batch 6800 Loss 1.3258 Accuracy 0.8511\n",
      "Epoch 4 Batch 6900 Loss 1.3259 Accuracy 0.8511\n",
      "Epoch 4 Batch 7000 Loss 1.3251 Accuracy 0.8512\n",
      "Epoch 4 Batch 7100 Loss 1.3248 Accuracy 0.8512\n",
      "Epoch 4 Batch 7200 Loss 1.3250 Accuracy 0.8513\n",
      "Epoch 4 Batch 7300 Loss 1.3246 Accuracy 0.8513\n",
      "Epoch 4 Batch 7400 Loss 1.3245 Accuracy 0.8513\n",
      "Epoch 4 Batch 7500 Loss 1.3246 Accuracy 0.8513\n",
      "Epoch 4 Batch 7600 Loss 1.3242 Accuracy 0.8514\n",
      "Epoch 4 Batch 7700 Loss 1.3243 Accuracy 0.8514\n",
      "Epoch 4 Batch 7800 Loss 1.3237 Accuracy 0.8515\n",
      "Epoch 4 Batch 7900 Loss 1.3235 Accuracy 0.8515\n",
      "Epoch 4 Batch 8000 Loss 1.3240 Accuracy 0.8515\n",
      "Epoch 4 Batch 8100 Loss 1.3238 Accuracy 0.8516\n",
      "Epoch 4 Batch 8200 Loss 1.3240 Accuracy 0.8516\n",
      "Epoch 4 Batch 8300 Loss 1.3232 Accuracy 0.8517\n",
      "Epoch 4 Batch 8400 Loss 1.3230 Accuracy 0.8517\n",
      "Epoch 4 Batch 8500 Loss 1.3232 Accuracy 0.8517\n",
      "Epoch 4 Batch 8600 Loss 1.3230 Accuracy 0.8517\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-2\n",
      "val-dataset result: Loss 3.2001 Accuracy 0.7921\n",
      "Epoch 4 Loss 1.3225 Accuracy 0.8518\n",
      "Time taken for 1 epoch: 3484.36 secs\n",
      "\n",
      "Epoch 5 Batch 100 Loss 1.3811 Accuracy 0.8433\n",
      "Epoch 5 Batch 200 Loss 1.3345 Accuracy 0.8492\n",
      "Epoch 5 Batch 300 Loss 1.3130 Accuracy 0.8528\n",
      "Epoch 5 Batch 400 Loss 1.3215 Accuracy 0.8519\n",
      "Epoch 5 Batch 500 Loss 1.3162 Accuracy 0.8531\n",
      "Epoch 5 Batch 600 Loss 1.3143 Accuracy 0.8536\n",
      "Epoch 5 Batch 700 Loss 1.3130 Accuracy 0.8537\n",
      "Epoch 5 Batch 800 Loss 1.3102 Accuracy 0.8537\n",
      "Epoch 5 Batch 900 Loss 1.3166 Accuracy 0.8533\n",
      "Epoch 5 Batch 1000 Loss 1.3113 Accuracy 0.8542\n",
      "Epoch 5 Batch 1100 Loss 1.3107 Accuracy 0.8544\n",
      "Epoch 5 Batch 1200 Loss 1.3079 Accuracy 0.8548\n",
      "Epoch 5 Batch 1300 Loss 1.3063 Accuracy 0.8549\n",
      "Epoch 5 Batch 1400 Loss 1.3063 Accuracy 0.8549\n",
      "Epoch 5 Batch 1500 Loss 1.3057 Accuracy 0.8551\n",
      "Epoch 5 Batch 1600 Loss 1.3084 Accuracy 0.8549\n",
      "Epoch 5 Batch 1700 Loss 1.3055 Accuracy 0.8553\n",
      "Epoch 5 Batch 1800 Loss 1.3027 Accuracy 0.8556\n",
      "Epoch 5 Batch 1900 Loss 1.3020 Accuracy 0.8556\n",
      "Epoch 5 Batch 2000 Loss 1.3076 Accuracy 0.8550\n",
      "Epoch 5 Batch 2100 Loss 1.3078 Accuracy 0.8550\n",
      "Epoch 5 Batch 2200 Loss 1.3079 Accuracy 0.8550\n",
      "Epoch 5 Batch 2300 Loss 1.3094 Accuracy 0.8548\n",
      "Epoch 5 Batch 2400 Loss 1.3077 Accuracy 0.8550\n",
      "Epoch 5 Batch 2500 Loss 1.3057 Accuracy 0.8551\n",
      "Epoch 5 Batch 2600 Loss 1.3065 Accuracy 0.8551\n",
      "Epoch 5 Batch 2700 Loss 1.3065 Accuracy 0.8551\n",
      "Epoch 5 Batch 2800 Loss 1.3079 Accuracy 0.8550\n",
      "Epoch 5 Batch 2900 Loss 1.3072 Accuracy 0.8550\n",
      "Epoch 5 Batch 3000 Loss 1.3057 Accuracy 0.8551\n",
      "Epoch 5 Batch 3100 Loss 1.3056 Accuracy 0.8551\n",
      "Epoch 5 Batch 3200 Loss 1.3068 Accuracy 0.8550\n",
      "Epoch 5 Batch 3300 Loss 1.3071 Accuracy 0.8550\n",
      "Epoch 5 Batch 3400 Loss 1.3070 Accuracy 0.8551\n",
      "Epoch 5 Batch 3500 Loss 1.3063 Accuracy 0.8551\n",
      "Epoch 5 Batch 3600 Loss 1.3068 Accuracy 0.8550\n",
      "Epoch 5 Batch 3700 Loss 1.3068 Accuracy 0.8550\n",
      "Epoch 5 Batch 3800 Loss 1.3065 Accuracy 0.8551\n",
      "Epoch 5 Batch 3900 Loss 1.3056 Accuracy 0.8553\n",
      "Epoch 5 Batch 4000 Loss 1.3042 Accuracy 0.8554\n",
      "Epoch 5 Batch 4100 Loss 1.3030 Accuracy 0.8555\n",
      "Epoch 5 Batch 4200 Loss 1.3043 Accuracy 0.8554\n",
      "Epoch 5 Batch 4300 Loss 1.3029 Accuracy 0.8556\n",
      "Epoch 5 Batch 4400 Loss 1.3022 Accuracy 0.8557\n",
      "Epoch 5 Batch 4500 Loss 1.3024 Accuracy 0.8556\n",
      "Epoch 5 Batch 4600 Loss 1.3020 Accuracy 0.8557\n",
      "Epoch 5 Batch 4700 Loss 1.3028 Accuracy 0.8556\n",
      "Epoch 5 Batch 4800 Loss 1.3022 Accuracy 0.8558\n",
      "Epoch 5 Batch 4900 Loss 1.3025 Accuracy 0.8557\n",
      "Epoch 5 Batch 5000 Loss 1.3024 Accuracy 0.8558\n",
      "Epoch 5 Batch 5100 Loss 1.3013 Accuracy 0.8559\n",
      "Epoch 5 Batch 5200 Loss 1.3015 Accuracy 0.8559\n",
      "Epoch 5 Batch 5300 Loss 1.3016 Accuracy 0.8559\n",
      "Epoch 5 Batch 5400 Loss 1.3020 Accuracy 0.8559\n",
      "Epoch 5 Batch 5500 Loss 1.3022 Accuracy 0.8559\n",
      "Epoch 5 Batch 5600 Loss 1.3010 Accuracy 0.8560\n",
      "Epoch 5 Batch 5700 Loss 1.3004 Accuracy 0.8561\n",
      "Epoch 5 Batch 5800 Loss 1.3007 Accuracy 0.8560\n",
      "Epoch 5 Batch 5900 Loss 1.3012 Accuracy 0.8560\n",
      "Epoch 5 Batch 6000 Loss 1.3010 Accuracy 0.8561\n",
      "Epoch 5 Batch 6100 Loss 1.3009 Accuracy 0.8561\n",
      "Epoch 5 Batch 6200 Loss 1.3002 Accuracy 0.8562\n",
      "Epoch 5 Batch 6300 Loss 1.3000 Accuracy 0.8562\n",
      "Epoch 5 Batch 6400 Loss 1.3002 Accuracy 0.8562\n",
      "Epoch 5 Batch 6500 Loss 1.3014 Accuracy 0.8561\n",
      "Epoch 5 Batch 6600 Loss 1.3016 Accuracy 0.8561\n",
      "Epoch 5 Batch 6700 Loss 1.3011 Accuracy 0.8562\n",
      "Epoch 5 Batch 6800 Loss 1.2997 Accuracy 0.8563\n",
      "Epoch 5 Batch 6900 Loss 1.3003 Accuracy 0.8563\n",
      "Epoch 5 Batch 7000 Loss 1.3000 Accuracy 0.8563\n",
      "Epoch 5 Batch 7100 Loss 1.3003 Accuracy 0.8563\n",
      "Epoch 5 Batch 7200 Loss 1.3003 Accuracy 0.8563\n",
      "Epoch 5 Batch 7300 Loss 1.2997 Accuracy 0.8563\n",
      "Epoch 5 Batch 7400 Loss 1.3006 Accuracy 0.8562\n",
      "Epoch 5 Batch 7500 Loss 1.3005 Accuracy 0.8563\n",
      "Epoch 5 Batch 7600 Loss 1.3012 Accuracy 0.8562\n",
      "Epoch 5 Batch 7700 Loss 1.3011 Accuracy 0.8563\n",
      "Epoch 5 Batch 7800 Loss 1.3005 Accuracy 0.8563\n",
      "Epoch 5 Batch 7900 Loss 1.3001 Accuracy 0.8564\n",
      "Epoch 5 Batch 8000 Loss 1.3000 Accuracy 0.8564\n",
      "Epoch 5 Batch 8100 Loss 1.3003 Accuracy 0.8564\n",
      "Epoch 5 Batch 8200 Loss 1.3002 Accuracy 0.8564\n",
      "Epoch 5 Batch 8300 Loss 1.3000 Accuracy 0.8564\n",
      "Epoch 5 Batch 8400 Loss 1.2995 Accuracy 0.8564\n",
      "Epoch 5 Batch 8500 Loss 1.2993 Accuracy 0.8565\n",
      "Epoch 5 Batch 8600 Loss 1.2989 Accuracy 0.8566\n",
      "Epoch 5 Loss 1.2986 Accuracy 0.8566\n",
      "Time taken for 1 epoch: 2269.04 secs\n",
      "\n",
      "Epoch 6 Batch 100 Loss 1.3768 Accuracy 0.8485\n",
      "Epoch 6 Batch 200 Loss 1.3257 Accuracy 0.8554\n",
      "Epoch 6 Batch 300 Loss 1.3153 Accuracy 0.8560\n",
      "Epoch 6 Batch 400 Loss 1.3273 Accuracy 0.8545\n",
      "Epoch 6 Batch 500 Loss 1.3232 Accuracy 0.8551\n",
      "Epoch 6 Batch 600 Loss 1.3203 Accuracy 0.8556\n",
      "Epoch 6 Batch 700 Loss 1.3152 Accuracy 0.8562\n",
      "Epoch 6 Batch 800 Loss 1.3063 Accuracy 0.8570\n",
      "Epoch 6 Batch 900 Loss 1.3066 Accuracy 0.8570\n",
      "Epoch 6 Batch 1000 Loss 1.3018 Accuracy 0.8577\n",
      "Epoch 6 Batch 1100 Loss 1.2991 Accuracy 0.8580\n",
      "Epoch 6 Batch 1200 Loss 1.2959 Accuracy 0.8584\n",
      "Epoch 6 Batch 1300 Loss 1.2962 Accuracy 0.8581\n",
      "Epoch 6 Batch 1400 Loss 1.2963 Accuracy 0.8580\n",
      "Epoch 6 Batch 1500 Loss 1.2969 Accuracy 0.8581\n",
      "Epoch 6 Batch 1600 Loss 1.2980 Accuracy 0.8580\n",
      "Epoch 6 Batch 1700 Loss 1.2982 Accuracy 0.8581\n",
      "Epoch 6 Batch 1800 Loss 1.2968 Accuracy 0.8583\n",
      "Epoch 6 Batch 1900 Loss 1.2960 Accuracy 0.8583\n",
      "Epoch 6 Batch 2000 Loss 1.2957 Accuracy 0.8584\n",
      "Epoch 6 Batch 2100 Loss 1.2955 Accuracy 0.8584\n",
      "Epoch 6 Batch 2200 Loss 1.2948 Accuracy 0.8586\n",
      "Epoch 6 Batch 2300 Loss 1.2957 Accuracy 0.8586\n",
      "Epoch 6 Batch 2400 Loss 1.2952 Accuracy 0.8587\n",
      "Epoch 6 Batch 2500 Loss 1.2950 Accuracy 0.8587\n",
      "Epoch 6 Batch 2600 Loss 1.2943 Accuracy 0.8588\n",
      "Epoch 6 Batch 2700 Loss 1.2938 Accuracy 0.8589\n",
      "Epoch 6 Batch 2800 Loss 1.2927 Accuracy 0.8590\n",
      "Epoch 6 Batch 2900 Loss 1.2920 Accuracy 0.8590\n",
      "Epoch 6 Batch 3000 Loss 1.2908 Accuracy 0.8591\n",
      "Epoch 6 Batch 3100 Loss 1.2911 Accuracy 0.8591\n",
      "Epoch 6 Batch 3200 Loss 1.2903 Accuracy 0.8592\n",
      "Epoch 6 Batch 3300 Loss 1.2909 Accuracy 0.8591\n",
      "Epoch 6 Batch 3400 Loss 1.2903 Accuracy 0.8592\n",
      "Epoch 6 Batch 3500 Loss 1.2904 Accuracy 0.8592\n",
      "Epoch 6 Batch 3600 Loss 1.2898 Accuracy 0.8593\n",
      "Epoch 6 Batch 3700 Loss 1.2887 Accuracy 0.8595\n",
      "Epoch 6 Batch 3800 Loss 1.2884 Accuracy 0.8594\n",
      "Epoch 6 Batch 3900 Loss 1.2888 Accuracy 0.8594\n",
      "Epoch 6 Batch 4000 Loss 1.2885 Accuracy 0.8595\n",
      "Epoch 6 Batch 4100 Loss 1.2890 Accuracy 0.8594\n",
      "Epoch 6 Batch 4200 Loss 1.2897 Accuracy 0.8594\n",
      "Epoch 6 Batch 4300 Loss 1.2880 Accuracy 0.8595\n",
      "Epoch 6 Batch 4400 Loss 1.2879 Accuracy 0.8596\n",
      "Epoch 6 Batch 4500 Loss 1.2875 Accuracy 0.8596\n",
      "Epoch 6 Batch 4600 Loss 1.2869 Accuracy 0.8596\n",
      "Epoch 6 Batch 4700 Loss 1.2866 Accuracy 0.8596\n",
      "Epoch 6 Batch 4800 Loss 1.2855 Accuracy 0.8598\n",
      "Epoch 6 Batch 4900 Loss 1.2864 Accuracy 0.8596\n",
      "Epoch 6 Batch 5000 Loss 1.2868 Accuracy 0.8596\n",
      "Epoch 6 Batch 5100 Loss 1.2866 Accuracy 0.8596\n",
      "Epoch 6 Batch 5200 Loss 1.2858 Accuracy 0.8597\n",
      "Epoch 6 Batch 5300 Loss 1.2865 Accuracy 0.8596\n",
      "Epoch 6 Batch 5400 Loss 1.2858 Accuracy 0.8597\n",
      "Epoch 6 Batch 5500 Loss 1.2854 Accuracy 0.8598\n",
      "Epoch 6 Batch 5600 Loss 1.2849 Accuracy 0.8599\n",
      "Epoch 6 Batch 5700 Loss 1.2842 Accuracy 0.8600\n",
      "Epoch 6 Batch 5800 Loss 1.2856 Accuracy 0.8598\n",
      "Epoch 6 Batch 5900 Loss 1.2847 Accuracy 0.8599\n",
      "Epoch 6 Batch 6000 Loss 1.2846 Accuracy 0.8599\n",
      "Epoch 6 Batch 6100 Loss 1.2848 Accuracy 0.8599\n",
      "Epoch 6 Batch 6200 Loss 1.2842 Accuracy 0.8599\n",
      "Epoch 6 Batch 6300 Loss 1.2849 Accuracy 0.8598\n",
      "Epoch 6 Batch 6400 Loss 1.2851 Accuracy 0.8598\n",
      "Epoch 6 Batch 6500 Loss 1.2849 Accuracy 0.8599\n",
      "Epoch 6 Batch 6600 Loss 1.2849 Accuracy 0.8599\n",
      "Epoch 6 Batch 6700 Loss 1.2841 Accuracy 0.8600\n",
      "Epoch 6 Batch 6800 Loss 1.2829 Accuracy 0.8601\n",
      "Epoch 6 Batch 6900 Loss 1.2840 Accuracy 0.8600\n",
      "Epoch 6 Batch 7000 Loss 1.2834 Accuracy 0.8601\n",
      "Epoch 6 Batch 7100 Loss 1.2840 Accuracy 0.8601\n",
      "Epoch 6 Batch 7200 Loss 1.2835 Accuracy 0.8601\n",
      "Epoch 6 Batch 7300 Loss 1.2828 Accuracy 0.8601\n",
      "Epoch 6 Batch 7400 Loss 1.2823 Accuracy 0.8602\n",
      "Epoch 6 Batch 7500 Loss 1.2824 Accuracy 0.8602\n",
      "Epoch 6 Batch 7600 Loss 1.2822 Accuracy 0.8603\n",
      "Epoch 6 Batch 7700 Loss 1.2824 Accuracy 0.8603\n",
      "Epoch 6 Batch 7800 Loss 1.2827 Accuracy 0.8602\n",
      "Epoch 6 Batch 7900 Loss 1.2825 Accuracy 0.8602\n",
      "Epoch 6 Batch 8000 Loss 1.2828 Accuracy 0.8602\n",
      "Epoch 6 Batch 8100 Loss 1.2825 Accuracy 0.8603\n",
      "Epoch 6 Batch 8200 Loss 1.2828 Accuracy 0.8603\n",
      "Epoch 6 Batch 8300 Loss 1.2826 Accuracy 0.8603\n",
      "Epoch 6 Batch 8400 Loss 1.2821 Accuracy 0.8603\n",
      "Epoch 6 Batch 8500 Loss 1.2820 Accuracy 0.8603\n",
      "Epoch 6 Batch 8600 Loss 1.2817 Accuracy 0.8604\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-3\n",
      "val-dataset result: Loss 3.2267 Accuracy 0.7956\n",
      "Epoch 6 Loss 1.2811 Accuracy 0.8605\n",
      "Time taken for 1 epoch: 3487.62 secs\n",
      "\n",
      "Epoch 7 Batch 100 Loss 1.3701 Accuracy 0.8526\n",
      "Epoch 7 Batch 200 Loss 1.3111 Accuracy 0.8589\n",
      "Epoch 7 Batch 300 Loss 1.2973 Accuracy 0.8596\n",
      "Epoch 7 Batch 400 Loss 1.2993 Accuracy 0.8594\n",
      "Epoch 7 Batch 500 Loss 1.3004 Accuracy 0.8592\n",
      "Epoch 7 Batch 600 Loss 1.2961 Accuracy 0.8600\n",
      "Epoch 7 Batch 700 Loss 1.2940 Accuracy 0.8598\n",
      "Epoch 7 Batch 800 Loss 1.2875 Accuracy 0.8603\n",
      "Epoch 7 Batch 900 Loss 1.2884 Accuracy 0.8601\n",
      "Epoch 7 Batch 1000 Loss 1.2902 Accuracy 0.8600\n",
      "Epoch 7 Batch 1100 Loss 1.2876 Accuracy 0.8603\n",
      "Epoch 7 Batch 1200 Loss 1.2875 Accuracy 0.8606\n",
      "Epoch 7 Batch 1300 Loss 1.2854 Accuracy 0.8605\n",
      "Epoch 7 Batch 1400 Loss 1.2833 Accuracy 0.8608\n",
      "Epoch 7 Batch 1500 Loss 1.2834 Accuracy 0.8609\n",
      "Epoch 7 Batch 1600 Loss 1.2827 Accuracy 0.8610\n",
      "Epoch 7 Batch 1700 Loss 1.2812 Accuracy 0.8613\n",
      "Epoch 7 Batch 1800 Loss 1.2803 Accuracy 0.8615\n",
      "Epoch 7 Batch 1900 Loss 1.2770 Accuracy 0.8618\n",
      "Epoch 7 Batch 2000 Loss 1.2807 Accuracy 0.8613\n",
      "Epoch 7 Batch 2100 Loss 1.2801 Accuracy 0.8614\n",
      "Epoch 7 Batch 2200 Loss 1.2813 Accuracy 0.8613\n",
      "Epoch 7 Batch 2300 Loss 1.2822 Accuracy 0.8612\n",
      "Epoch 7 Batch 2400 Loss 1.2792 Accuracy 0.8615\n",
      "Epoch 7 Batch 2500 Loss 1.2787 Accuracy 0.8615\n",
      "Epoch 7 Batch 2600 Loss 1.2785 Accuracy 0.8616\n",
      "Epoch 7 Batch 2700 Loss 1.2779 Accuracy 0.8616\n",
      "Epoch 7 Batch 2800 Loss 1.2791 Accuracy 0.8616\n",
      "Epoch 7 Batch 2900 Loss 1.2800 Accuracy 0.8614\n",
      "Epoch 7 Batch 3000 Loss 1.2788 Accuracy 0.8615\n",
      "Epoch 7 Batch 3100 Loss 1.2795 Accuracy 0.8614\n",
      "Epoch 7 Batch 3200 Loss 1.2799 Accuracy 0.8614\n",
      "Epoch 7 Batch 3300 Loss 1.2802 Accuracy 0.8614\n",
      "Epoch 7 Batch 3400 Loss 1.2798 Accuracy 0.8615\n",
      "Epoch 7 Batch 3500 Loss 1.2775 Accuracy 0.8617\n",
      "Epoch 7 Batch 3600 Loss 1.2785 Accuracy 0.8616\n",
      "Epoch 7 Batch 3700 Loss 1.2785 Accuracy 0.8617\n",
      "Epoch 7 Batch 3800 Loss 1.2778 Accuracy 0.8618\n",
      "Epoch 7 Batch 3900 Loss 1.2777 Accuracy 0.8618\n",
      "Epoch 7 Batch 4000 Loss 1.2764 Accuracy 0.8619\n",
      "Epoch 7 Batch 4100 Loss 1.2764 Accuracy 0.8619\n",
      "Epoch 7 Batch 4200 Loss 1.2768 Accuracy 0.8619\n",
      "Epoch 7 Batch 4300 Loss 1.2774 Accuracy 0.8618\n",
      "Epoch 7 Batch 4400 Loss 1.2774 Accuracy 0.8618\n",
      "Epoch 7 Batch 4500 Loss 1.2768 Accuracy 0.8618\n",
      "Epoch 7 Batch 4600 Loss 1.2763 Accuracy 0.8618\n",
      "Epoch 7 Batch 4700 Loss 1.2773 Accuracy 0.8617\n",
      "Epoch 7 Batch 4800 Loss 1.2770 Accuracy 0.8618\n",
      "Epoch 7 Batch 4900 Loss 1.2764 Accuracy 0.8619\n",
      "Epoch 7 Batch 5000 Loss 1.2769 Accuracy 0.8618\n",
      "Epoch 7 Batch 5100 Loss 1.2763 Accuracy 0.8619\n",
      "Epoch 7 Batch 5200 Loss 1.2759 Accuracy 0.8619\n",
      "Epoch 7 Batch 5300 Loss 1.2761 Accuracy 0.8619\n",
      "Epoch 7 Batch 5400 Loss 1.2761 Accuracy 0.8619\n",
      "Epoch 7 Batch 5500 Loss 1.2768 Accuracy 0.8618\n",
      "Epoch 7 Batch 5600 Loss 1.2765 Accuracy 0.8618\n",
      "Epoch 7 Batch 5700 Loss 1.2762 Accuracy 0.8618\n",
      "Epoch 7 Batch 5800 Loss 1.2766 Accuracy 0.8618\n",
      "Epoch 7 Batch 5900 Loss 1.2760 Accuracy 0.8618\n",
      "Epoch 7 Batch 6000 Loss 1.2763 Accuracy 0.8618\n",
      "Epoch 7 Batch 6100 Loss 1.2765 Accuracy 0.8618\n",
      "Epoch 7 Batch 6200 Loss 1.2762 Accuracy 0.8618\n",
      "Epoch 7 Batch 6300 Loss 1.2758 Accuracy 0.8619\n",
      "Epoch 7 Batch 6400 Loss 1.2760 Accuracy 0.8619\n",
      "Epoch 7 Batch 6500 Loss 1.2754 Accuracy 0.8620\n",
      "Epoch 7 Batch 6600 Loss 1.2753 Accuracy 0.8620\n",
      "Epoch 7 Batch 6700 Loss 1.2749 Accuracy 0.8620\n",
      "Epoch 7 Batch 6800 Loss 1.2744 Accuracy 0.8621\n",
      "Epoch 7 Batch 6900 Loss 1.2747 Accuracy 0.8621\n",
      "Epoch 7 Batch 7000 Loss 1.2749 Accuracy 0.8620\n",
      "Epoch 7 Batch 7100 Loss 1.2753 Accuracy 0.8620\n",
      "Epoch 7 Batch 7200 Loss 1.2750 Accuracy 0.8621\n",
      "Epoch 7 Batch 7300 Loss 1.2747 Accuracy 0.8621\n",
      "Epoch 7 Batch 7400 Loss 1.2747 Accuracy 0.8620\n",
      "Epoch 7 Batch 7500 Loss 1.2748 Accuracy 0.8620\n",
      "Epoch 7 Batch 7600 Loss 1.2752 Accuracy 0.8620\n",
      "Epoch 7 Batch 7700 Loss 1.2756 Accuracy 0.8620\n",
      "Epoch 7 Batch 7800 Loss 1.2753 Accuracy 0.8620\n",
      "Epoch 7 Batch 7900 Loss 1.2752 Accuracy 0.8620\n",
      "Epoch 7 Batch 8000 Loss 1.2755 Accuracy 0.8620\n",
      "Epoch 7 Batch 8100 Loss 1.2754 Accuracy 0.8620\n",
      "Epoch 7 Batch 8200 Loss 1.2753 Accuracy 0.8621\n",
      "Epoch 7 Batch 8300 Loss 1.2755 Accuracy 0.8620\n",
      "Epoch 7 Batch 8400 Loss 1.2748 Accuracy 0.8621\n",
      "Epoch 7 Batch 8500 Loss 1.2749 Accuracy 0.8621\n",
      "Epoch 7 Batch 8600 Loss 1.2749 Accuracy 0.8621\n",
      "Epoch 7 Loss 1.2744 Accuracy 0.8622\n",
      "Time taken for 1 epoch: 2272.50 secs\n",
      "\n",
      "Epoch 8 Batch 100 Loss 1.3580 Accuracy 0.8546\n",
      "Epoch 8 Batch 200 Loss 1.2934 Accuracy 0.8609\n",
      "Epoch 8 Batch 300 Loss 1.2856 Accuracy 0.8620\n",
      "Epoch 8 Batch 400 Loss 1.2955 Accuracy 0.8616\n",
      "Epoch 8 Batch 500 Loss 1.2956 Accuracy 0.8609\n",
      "Epoch 8 Batch 600 Loss 1.2881 Accuracy 0.8624\n",
      "Epoch 8 Batch 700 Loss 1.2895 Accuracy 0.8622\n",
      "Epoch 8 Batch 800 Loss 1.2800 Accuracy 0.8630\n",
      "Epoch 8 Batch 900 Loss 1.2787 Accuracy 0.8630\n",
      "Epoch 8 Batch 1000 Loss 1.2796 Accuracy 0.8628\n",
      "Epoch 8 Batch 1100 Loss 1.2756 Accuracy 0.8634\n",
      "Epoch 8 Batch 1200 Loss 1.2758 Accuracy 0.8634\n",
      "Epoch 8 Batch 1300 Loss 1.2748 Accuracy 0.8635\n",
      "Epoch 8 Batch 1400 Loss 1.2760 Accuracy 0.8634\n",
      "Epoch 8 Batch 1500 Loss 1.2770 Accuracy 0.8634\n",
      "Epoch 8 Batch 1600 Loss 1.2746 Accuracy 0.8637\n",
      "Epoch 8 Batch 1700 Loss 1.2767 Accuracy 0.8635\n",
      "Epoch 8 Batch 1800 Loss 1.2747 Accuracy 0.8636\n",
      "Epoch 8 Batch 1900 Loss 1.2737 Accuracy 0.8637\n",
      "Epoch 8 Batch 2000 Loss 1.2732 Accuracy 0.8637\n",
      "Epoch 8 Batch 2100 Loss 1.2742 Accuracy 0.8636\n",
      "Epoch 8 Batch 2200 Loss 1.2729 Accuracy 0.8638\n",
      "Epoch 8 Batch 2300 Loss 1.2724 Accuracy 0.8638\n",
      "Epoch 8 Batch 2400 Loss 1.2702 Accuracy 0.8640\n",
      "Epoch 8 Batch 2500 Loss 1.2676 Accuracy 0.8642\n",
      "Epoch 8 Batch 2600 Loss 1.2679 Accuracy 0.8641\n",
      "Epoch 8 Batch 2700 Loss 1.2681 Accuracy 0.8640\n",
      "Epoch 8 Batch 2800 Loss 1.2694 Accuracy 0.8639\n",
      "Epoch 8 Batch 2900 Loss 1.2694 Accuracy 0.8638\n",
      "Epoch 8 Batch 3000 Loss 1.2678 Accuracy 0.8639\n",
      "Epoch 8 Batch 3100 Loss 1.2678 Accuracy 0.8640\n",
      "Epoch 8 Batch 3200 Loss 1.2667 Accuracy 0.8641\n",
      "Epoch 8 Batch 3300 Loss 1.2676 Accuracy 0.8640\n",
      "Epoch 8 Batch 3400 Loss 1.2680 Accuracy 0.8640\n",
      "Epoch 8 Batch 3500 Loss 1.2654 Accuracy 0.8643\n",
      "Epoch 8 Batch 3600 Loss 1.2657 Accuracy 0.8643\n",
      "Epoch 8 Batch 3700 Loss 1.2660 Accuracy 0.8643\n",
      "Epoch 8 Batch 3800 Loss 1.2660 Accuracy 0.8643\n",
      "Epoch 8 Batch 3900 Loss 1.2672 Accuracy 0.8642\n",
      "Epoch 8 Batch 4000 Loss 1.2661 Accuracy 0.8643\n",
      "Epoch 8 Batch 4100 Loss 1.2659 Accuracy 0.8643\n",
      "Epoch 8 Batch 4200 Loss 1.2671 Accuracy 0.8641\n",
      "Epoch 8 Batch 4300 Loss 1.2666 Accuracy 0.8642\n",
      "Epoch 8 Batch 4400 Loss 1.2673 Accuracy 0.8641\n",
      "Epoch 8 Batch 4500 Loss 1.2670 Accuracy 0.8642\n",
      "Epoch 8 Batch 4600 Loss 1.2663 Accuracy 0.8643\n",
      "Epoch 8 Batch 4700 Loss 1.2654 Accuracy 0.8643\n",
      "Epoch 8 Batch 4800 Loss 1.2653 Accuracy 0.8644\n",
      "Epoch 8 Batch 4900 Loss 1.2648 Accuracy 0.8644\n",
      "Epoch 8 Batch 5000 Loss 1.2661 Accuracy 0.8643\n",
      "Epoch 8 Batch 5100 Loss 1.2652 Accuracy 0.8644\n",
      "Epoch 8 Batch 5200 Loss 1.2646 Accuracy 0.8644\n",
      "Epoch 8 Batch 5300 Loss 1.2653 Accuracy 0.8644\n",
      "Epoch 8 Batch 5400 Loss 1.2652 Accuracy 0.8644\n",
      "Epoch 8 Batch 5500 Loss 1.2661 Accuracy 0.8643\n",
      "Epoch 8 Batch 5600 Loss 1.2646 Accuracy 0.8645\n",
      "Epoch 8 Batch 5700 Loss 1.2647 Accuracy 0.8645\n",
      "Epoch 8 Batch 5800 Loss 1.2652 Accuracy 0.8645\n",
      "Epoch 8 Batch 5900 Loss 1.2648 Accuracy 0.8646\n",
      "Epoch 8 Batch 6000 Loss 1.2648 Accuracy 0.8646\n",
      "Epoch 8 Batch 6100 Loss 1.2649 Accuracy 0.8646\n",
      "Epoch 8 Batch 6200 Loss 1.2645 Accuracy 0.8646\n",
      "Epoch 8 Batch 6300 Loss 1.2642 Accuracy 0.8646\n",
      "Epoch 8 Batch 6400 Loss 1.2642 Accuracy 0.8646\n",
      "Epoch 8 Batch 6500 Loss 1.2642 Accuracy 0.8646\n",
      "Epoch 8 Batch 6600 Loss 1.2652 Accuracy 0.8645\n",
      "Epoch 8 Batch 6700 Loss 1.2649 Accuracy 0.8646\n",
      "Epoch 8 Batch 6800 Loss 1.2645 Accuracy 0.8646\n",
      "Epoch 8 Batch 6900 Loss 1.2651 Accuracy 0.8645\n",
      "Epoch 8 Batch 7000 Loss 1.2651 Accuracy 0.8645\n",
      "Epoch 8 Batch 7100 Loss 1.2648 Accuracy 0.8646\n",
      "Epoch 8 Batch 7200 Loss 1.2651 Accuracy 0.8646\n",
      "Epoch 8 Batch 7300 Loss 1.2649 Accuracy 0.8646\n",
      "Epoch 8 Batch 7400 Loss 1.2648 Accuracy 0.8646\n",
      "Epoch 8 Batch 7500 Loss 1.2645 Accuracy 0.8646\n",
      "Epoch 8 Batch 7600 Loss 1.2647 Accuracy 0.8646\n",
      "Epoch 8 Batch 7700 Loss 1.2646 Accuracy 0.8647\n",
      "Epoch 8 Batch 7800 Loss 1.2643 Accuracy 0.8647\n",
      "Epoch 8 Batch 7900 Loss 1.2644 Accuracy 0.8646\n",
      "Epoch 8 Batch 8000 Loss 1.2648 Accuracy 0.8646\n",
      "Epoch 8 Batch 8100 Loss 1.2646 Accuracy 0.8646\n",
      "Epoch 8 Batch 8200 Loss 1.2657 Accuracy 0.8645\n",
      "Epoch 8 Batch 8300 Loss 1.2657 Accuracy 0.8646\n",
      "Epoch 8 Batch 8400 Loss 1.2647 Accuracy 0.8647\n",
      "Epoch 8 Batch 8500 Loss 1.2649 Accuracy 0.8646\n",
      "Epoch 8 Batch 8600 Loss 1.2648 Accuracy 0.8647\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-4\n",
      "val-dataset result: Loss 3.1279 Accuracy 0.7994\n",
      "Epoch 8 Loss 1.2642 Accuracy 0.8648\n",
      "Time taken for 1 epoch: 3391.91 secs\n",
      "\n",
      "Epoch 9 Batch 100 Loss 1.3590 Accuracy 0.8517\n",
      "Epoch 9 Batch 200 Loss 1.2775 Accuracy 0.8619\n",
      "Epoch 9 Batch 300 Loss 1.2712 Accuracy 0.8624\n",
      "Epoch 9 Batch 400 Loss 1.2752 Accuracy 0.8634\n",
      "Epoch 9 Batch 500 Loss 1.2760 Accuracy 0.8636\n",
      "Epoch 9 Batch 600 Loss 1.2674 Accuracy 0.8643\n",
      "Epoch 9 Batch 700 Loss 1.2670 Accuracy 0.8644\n",
      "Epoch 9 Batch 800 Loss 1.2616 Accuracy 0.8650\n",
      "Epoch 9 Batch 900 Loss 1.2655 Accuracy 0.8647\n",
      "Epoch 9 Batch 1000 Loss 1.2670 Accuracy 0.8647\n",
      "Epoch 9 Batch 1100 Loss 1.2645 Accuracy 0.8650\n",
      "Epoch 9 Batch 1200 Loss 1.2640 Accuracy 0.8651\n",
      "Epoch 9 Batch 1300 Loss 1.2597 Accuracy 0.8655\n",
      "Epoch 9 Batch 1400 Loss 1.2612 Accuracy 0.8652\n",
      "Epoch 9 Batch 1500 Loss 1.2620 Accuracy 0.8653\n",
      "Epoch 9 Batch 1600 Loss 1.2623 Accuracy 0.8652\n",
      "Epoch 9 Batch 1700 Loss 1.2633 Accuracy 0.8651\n",
      "Epoch 9 Batch 1800 Loss 1.2641 Accuracy 0.8651\n",
      "Epoch 9 Batch 1900 Loss 1.2616 Accuracy 0.8652\n",
      "Epoch 9 Batch 2000 Loss 1.2641 Accuracy 0.8650\n",
      "Epoch 9 Batch 2100 Loss 1.2629 Accuracy 0.8653\n",
      "Epoch 9 Batch 2200 Loss 1.2616 Accuracy 0.8655\n",
      "Epoch 9 Batch 2300 Loss 1.2596 Accuracy 0.8658\n",
      "Epoch 9 Batch 2400 Loss 1.2588 Accuracy 0.8658\n",
      "Epoch 9 Batch 2500 Loss 1.2573 Accuracy 0.8659\n",
      "Epoch 9 Batch 2600 Loss 1.2587 Accuracy 0.8658\n",
      "Epoch 9 Batch 2700 Loss 1.2578 Accuracy 0.8659\n",
      "Epoch 9 Batch 2800 Loss 1.2585 Accuracy 0.8658\n",
      "Epoch 9 Batch 2900 Loss 1.2572 Accuracy 0.8660\n",
      "Epoch 9 Batch 3000 Loss 1.2567 Accuracy 0.8662\n",
      "Epoch 9 Batch 3100 Loss 1.2589 Accuracy 0.8659\n",
      "Epoch 9 Batch 3200 Loss 1.2584 Accuracy 0.8660\n",
      "Epoch 9 Batch 3300 Loss 1.2574 Accuracy 0.8662\n",
      "Epoch 9 Batch 3400 Loss 1.2577 Accuracy 0.8661\n",
      "Epoch 9 Batch 3500 Loss 1.2553 Accuracy 0.8663\n",
      "Epoch 9 Batch 3600 Loss 1.2554 Accuracy 0.8662\n",
      "Epoch 9 Batch 3700 Loss 1.2562 Accuracy 0.8662\n",
      "Epoch 9 Batch 3800 Loss 1.2554 Accuracy 0.8664\n",
      "Epoch 9 Batch 3900 Loss 1.2570 Accuracy 0.8662\n",
      "Epoch 9 Batch 4000 Loss 1.2555 Accuracy 0.8664\n",
      "Epoch 9 Batch 4100 Loss 1.2542 Accuracy 0.8664\n",
      "Epoch 9 Batch 4200 Loss 1.2543 Accuracy 0.8664\n",
      "Epoch 9 Batch 4300 Loss 1.2557 Accuracy 0.8662\n",
      "Epoch 9 Batch 4400 Loss 1.2552 Accuracy 0.8663\n",
      "Epoch 9 Batch 4500 Loss 1.2559 Accuracy 0.8663\n",
      "Epoch 9 Batch 4600 Loss 1.2559 Accuracy 0.8662\n",
      "Epoch 9 Batch 4700 Loss 1.2564 Accuracy 0.8661\n",
      "Epoch 9 Batch 4800 Loss 1.2563 Accuracy 0.8661\n",
      "Epoch 9 Batch 4900 Loss 1.2565 Accuracy 0.8661\n",
      "Epoch 9 Batch 5000 Loss 1.2562 Accuracy 0.8662\n",
      "Epoch 9 Batch 5100 Loss 1.2561 Accuracy 0.8662\n",
      "Epoch 9 Batch 5200 Loss 1.2555 Accuracy 0.8663\n",
      "Epoch 9 Batch 5300 Loss 1.2570 Accuracy 0.8661\n",
      "Epoch 9 Batch 5400 Loss 1.2566 Accuracy 0.8662\n",
      "Epoch 9 Batch 5500 Loss 1.2565 Accuracy 0.8662\n",
      "Epoch 9 Batch 5600 Loss 1.2557 Accuracy 0.8663\n",
      "Epoch 9 Batch 5700 Loss 1.2549 Accuracy 0.8663\n",
      "Epoch 9 Batch 5800 Loss 1.2551 Accuracy 0.8663\n",
      "Epoch 9 Batch 5900 Loss 1.2554 Accuracy 0.8663\n",
      "Epoch 9 Batch 6000 Loss 1.2557 Accuracy 0.8663\n",
      "Epoch 9 Batch 6100 Loss 1.2555 Accuracy 0.8663\n",
      "Epoch 9 Batch 6200 Loss 1.2555 Accuracy 0.8663\n",
      "Epoch 9 Batch 6300 Loss 1.2552 Accuracy 0.8663\n",
      "Epoch 9 Batch 6400 Loss 1.2553 Accuracy 0.8664\n",
      "Epoch 9 Batch 6500 Loss 1.2557 Accuracy 0.8663\n",
      "Epoch 9 Batch 6600 Loss 1.2566 Accuracy 0.8662\n",
      "Epoch 9 Batch 6700 Loss 1.2566 Accuracy 0.8662\n",
      "Epoch 9 Batch 6800 Loss 1.2563 Accuracy 0.8663\n",
      "Epoch 9 Batch 6900 Loss 1.2563 Accuracy 0.8663\n",
      "Epoch 9 Batch 7000 Loss 1.2567 Accuracy 0.8662\n",
      "Epoch 9 Batch 7100 Loss 1.2570 Accuracy 0.8662\n",
      "Epoch 9 Batch 7200 Loss 1.2570 Accuracy 0.8663\n",
      "Epoch 9 Batch 7300 Loss 1.2564 Accuracy 0.8663\n",
      "Epoch 9 Batch 7400 Loss 1.2571 Accuracy 0.8663\n",
      "Epoch 9 Batch 7500 Loss 1.2570 Accuracy 0.8663\n",
      "Epoch 9 Batch 7600 Loss 1.2574 Accuracy 0.8663\n",
      "Epoch 9 Batch 7700 Loss 1.2581 Accuracy 0.8663\n",
      "Epoch 9 Batch 7800 Loss 1.2576 Accuracy 0.8663\n",
      "Epoch 9 Batch 7900 Loss 1.2575 Accuracy 0.8663\n",
      "Epoch 9 Batch 8000 Loss 1.2582 Accuracy 0.8663\n",
      "Epoch 9 Batch 8100 Loss 1.2581 Accuracy 0.8663\n",
      "Epoch 9 Batch 8200 Loss 1.2582 Accuracy 0.8663\n",
      "Epoch 9 Batch 8300 Loss 1.2581 Accuracy 0.8663\n",
      "Epoch 9 Batch 8400 Loss 1.2580 Accuracy 0.8663\n",
      "Epoch 9 Batch 8500 Loss 1.2580 Accuracy 0.8663\n",
      "Epoch 9 Batch 8600 Loss 1.2580 Accuracy 0.8664\n",
      "Epoch 9 Loss 1.2575 Accuracy 0.8664\n",
      "Time taken for 1 epoch: 2250.31 secs\n",
      "\n",
      "Epoch 10 Batch 100 Loss 1.3434 Accuracy 0.8577\n",
      "Epoch 10 Batch 200 Loss 1.3001 Accuracy 0.8621\n",
      "Epoch 10 Batch 300 Loss 1.2904 Accuracy 0.8631\n",
      "Epoch 10 Batch 400 Loss 1.2845 Accuracy 0.8637\n",
      "Epoch 10 Batch 500 Loss 1.2802 Accuracy 0.8639\n",
      "Epoch 10 Batch 600 Loss 1.2784 Accuracy 0.8646\n",
      "Epoch 10 Batch 700 Loss 1.2707 Accuracy 0.8653\n",
      "Epoch 10 Batch 800 Loss 1.2640 Accuracy 0.8661\n",
      "Epoch 10 Batch 900 Loss 1.2669 Accuracy 0.8662\n",
      "Epoch 10 Batch 1000 Loss 1.2658 Accuracy 0.8664\n",
      "Epoch 10 Batch 1100 Loss 1.2631 Accuracy 0.8666\n",
      "Epoch 10 Batch 1200 Loss 1.2653 Accuracy 0.8663\n",
      "Epoch 10 Batch 1300 Loss 1.2626 Accuracy 0.8664\n",
      "Epoch 10 Batch 1400 Loss 1.2618 Accuracy 0.8665\n",
      "Epoch 10 Batch 1500 Loss 1.2642 Accuracy 0.8665\n",
      "Epoch 10 Batch 1600 Loss 1.2636 Accuracy 0.8667\n",
      "Epoch 10 Batch 1700 Loss 1.2668 Accuracy 0.8664\n",
      "Epoch 10 Batch 1800 Loss 1.2672 Accuracy 0.8663\n",
      "Epoch 10 Batch 1900 Loss 1.2652 Accuracy 0.8664\n",
      "Epoch 10 Batch 2000 Loss 1.2644 Accuracy 0.8665\n",
      "Epoch 10 Batch 2100 Loss 1.2642 Accuracy 0.8666\n",
      "Epoch 10 Batch 2200 Loss 1.2627 Accuracy 0.8668\n",
      "Epoch 10 Batch 2300 Loss 1.2636 Accuracy 0.8666\n",
      "Epoch 10 Batch 2400 Loss 1.2628 Accuracy 0.8666\n",
      "Epoch 10 Batch 2500 Loss 1.2613 Accuracy 0.8669\n",
      "Epoch 10 Batch 2600 Loss 1.2613 Accuracy 0.8669\n",
      "Epoch 10 Batch 2700 Loss 1.2627 Accuracy 0.8667\n",
      "Epoch 10 Batch 2800 Loss 1.2621 Accuracy 0.8668\n",
      "Epoch 10 Batch 2900 Loss 1.2618 Accuracy 0.8668\n",
      "Epoch 10 Batch 3000 Loss 1.2608 Accuracy 0.8669\n",
      "Epoch 10 Batch 3100 Loss 1.2605 Accuracy 0.8669\n",
      "Epoch 10 Batch 3200 Loss 1.2610 Accuracy 0.8669\n",
      "Epoch 10 Batch 3300 Loss 1.2616 Accuracy 0.8669\n",
      "Epoch 10 Batch 3400 Loss 1.2599 Accuracy 0.8670\n",
      "Epoch 10 Batch 3500 Loss 1.2591 Accuracy 0.8671\n",
      "Epoch 10 Batch 3600 Loss 1.2600 Accuracy 0.8670\n",
      "Epoch 10 Batch 3700 Loss 1.2589 Accuracy 0.8672\n",
      "Epoch 10 Batch 3800 Loss 1.2598 Accuracy 0.8671\n",
      "Epoch 10 Batch 3900 Loss 1.2608 Accuracy 0.8670\n",
      "Epoch 10 Batch 4000 Loss 1.2601 Accuracy 0.8671\n",
      "Epoch 10 Batch 4100 Loss 1.2593 Accuracy 0.8672\n",
      "Epoch 10 Batch 4200 Loss 1.2608 Accuracy 0.8671\n",
      "Epoch 10 Batch 4300 Loss 1.2594 Accuracy 0.8672\n",
      "Epoch 10 Batch 4400 Loss 1.2598 Accuracy 0.8672\n",
      "Epoch 10 Batch 4500 Loss 1.2585 Accuracy 0.8673\n",
      "Epoch 10 Batch 4600 Loss 1.2573 Accuracy 0.8675\n",
      "Epoch 10 Batch 4700 Loss 1.2583 Accuracy 0.8674\n",
      "Epoch 10 Batch 4800 Loss 1.2583 Accuracy 0.8675\n",
      "Epoch 10 Batch 4900 Loss 1.2576 Accuracy 0.8676\n",
      "Epoch 10 Batch 5000 Loss 1.2581 Accuracy 0.8675\n",
      "Epoch 10 Batch 5100 Loss 1.2576 Accuracy 0.8676\n",
      "Epoch 10 Batch 5200 Loss 1.2578 Accuracy 0.8676\n",
      "Epoch 10 Batch 5300 Loss 1.2586 Accuracy 0.8675\n",
      "Epoch 10 Batch 5400 Loss 1.2580 Accuracy 0.8676\n",
      "Epoch 10 Batch 5500 Loss 1.2586 Accuracy 0.8676\n",
      "Epoch 10 Batch 5600 Loss 1.2581 Accuracy 0.8676\n",
      "Epoch 10 Batch 5700 Loss 1.2571 Accuracy 0.8677\n",
      "Epoch 10 Batch 5800 Loss 1.2573 Accuracy 0.8677\n",
      "Epoch 10 Batch 5900 Loss 1.2577 Accuracy 0.8676\n",
      "Epoch 10 Batch 6000 Loss 1.2569 Accuracy 0.8678\n",
      "Epoch 10 Batch 6100 Loss 1.2576 Accuracy 0.8677\n",
      "Epoch 10 Batch 6200 Loss 1.2571 Accuracy 0.8678\n",
      "Epoch 10 Batch 6300 Loss 1.2573 Accuracy 0.8677\n",
      "Epoch 10 Batch 6400 Loss 1.2576 Accuracy 0.8677\n",
      "Epoch 10 Batch 6500 Loss 1.2581 Accuracy 0.8677\n",
      "Epoch 10 Batch 6600 Loss 1.2585 Accuracy 0.8677\n",
      "Epoch 10 Batch 6700 Loss 1.2585 Accuracy 0.8677\n",
      "Epoch 10 Batch 6800 Loss 1.2579 Accuracy 0.8677\n",
      "Epoch 10 Batch 6900 Loss 1.2588 Accuracy 0.8676\n",
      "Epoch 10 Batch 7000 Loss 1.2587 Accuracy 0.8676\n",
      "Epoch 10 Batch 7100 Loss 1.2581 Accuracy 0.8677\n",
      "Epoch 10 Batch 7200 Loss 1.2578 Accuracy 0.8678\n",
      "Epoch 10 Batch 7300 Loss 1.2577 Accuracy 0.8677\n",
      "Epoch 10 Batch 7400 Loss 1.2572 Accuracy 0.8678\n",
      "Epoch 10 Batch 7500 Loss 1.2579 Accuracy 0.8677\n",
      "Epoch 10 Batch 7600 Loss 1.2578 Accuracy 0.8677\n",
      "Epoch 10 Batch 7700 Loss 1.2586 Accuracy 0.8677\n",
      "Epoch 10 Batch 7800 Loss 1.2578 Accuracy 0.8678\n",
      "Epoch 10 Batch 7900 Loss 1.2575 Accuracy 0.8678\n",
      "Epoch 10 Batch 8000 Loss 1.2576 Accuracy 0.8678\n",
      "Epoch 10 Batch 8100 Loss 1.2577 Accuracy 0.8678\n",
      "Epoch 10 Batch 8200 Loss 1.2581 Accuracy 0.8677\n",
      "Epoch 10 Batch 8300 Loss 1.2578 Accuracy 0.8678\n",
      "Epoch 10 Batch 8400 Loss 1.2575 Accuracy 0.8678\n",
      "Epoch 10 Batch 8500 Loss 1.2577 Accuracy 0.8677\n",
      "Epoch 10 Batch 8600 Loss 1.2576 Accuracy 0.8678\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-5\n",
      "val-dataset result: Loss 3.2154 Accuracy 0.8004\n",
      "Epoch 10 Loss 1.2574 Accuracy 0.8678\n",
      "Time taken for 1 epoch: 3390.85 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 210624, euni: random mask 추가\n",
    "import time\n",
    "train_loss_main = []\n",
    "val_loss_main = []\n",
    "train_acc_main = []\n",
    "val_acc_main = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % (EPOCHS//5) == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "        for inp, tar  in val_dataset:\n",
    "            val_step(inp, tar)\n",
    "\n",
    "        print(f'val-dataset result: Loss {val_loss.result():.4f} Accuracy {val_accuracy.result():.4f}')\n",
    "        train_loss_main.append(train_loss.result())\n",
    "        val_loss_main.append(val_loss.result())\n",
    "        train_acc_main.append(train_accuracy.result())\n",
    "        val_acc_main.append(val_accuracy.result())\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dimensional-narrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optimizer 별로인듯..ㅜ-> d_model 잘못넣었었음\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(val_acc_main) + 1)\n",
    "\n",
    "plt.plot(epochs, train_acc_main, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc_main, 'b', label = 'Validation accuracy')\n",
    "plt.title('Training and Validation categorical_accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_loss_main, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss_main, 'b', label = 'Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "regular-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer.save_weights(\"model/transformer_QA_weights_repeat.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-enclosure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-knife",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fluid-pitch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 100 Loss 10.2915 Accuracy 0.0421\n",
      "Epoch 1 Batch 200 Loss 10.0897 Accuracy 0.1105\n",
      "Epoch 1 Batch 300 Loss 9.7997 Accuracy 0.1305\n",
      "Epoch 1 Batch 400 Loss 9.4306 Accuracy 0.1369\n",
      "Epoch 1 Batch 500 Loss 8.9938 Accuracy 0.1446\n",
      "Epoch 1 Loss 8.7989 Accuracy 0.1476\n",
      "Time taken for 1 epoch: 140.87 secs\n",
      "\n",
      "Epoch 2 Batch 100 Loss 6.1746 Accuracy 0.2787\n",
      "Epoch 2 Batch 200 Loss 5.8148 Accuracy 0.3176\n",
      "Epoch 2 Batch 300 Loss 5.6058 Accuracy 0.3281\n",
      "Epoch 2 Batch 400 Loss 5.5107 Accuracy 0.3346\n",
      "Epoch 2 Batch 500 Loss 5.3553 Accuracy 0.3530\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train0/ckpt-1\n",
      "val-dataset result: Loss 4.9189 Accuracy 0.4375\n",
      "Epoch 2 Loss 5.2903 Accuracy 0.3606\n",
      "Time taken for 1 epoch: 1269.49 secs\n",
      "\n",
      "Epoch 3 Batch 100 Loss 4.5584 Accuracy 0.4257\n",
      "Epoch 3 Batch 200 Loss 4.3641 Accuracy 0.4492\n",
      "Epoch 3 Batch 300 Loss 4.2290 Accuracy 0.4623\n",
      "Epoch 3 Batch 400 Loss 4.1667 Accuracy 0.4679\n",
      "Epoch 3 Batch 500 Loss 4.0643 Accuracy 0.4793\n",
      "Epoch 3 Loss 4.0259 Accuracy 0.4839\n",
      "Time taken for 1 epoch: 142.04 secs\n",
      "\n",
      "Epoch 4 Batch 100 Loss 3.4722 Accuracy 0.5371\n",
      "Epoch 4 Batch 200 Loss 3.2658 Accuracy 0.5687\n",
      "Epoch 4 Batch 300 Loss 3.1497 Accuracy 0.5865\n",
      "Epoch 4 Batch 400 Loss 3.0851 Accuracy 0.5972\n",
      "Epoch 4 Batch 500 Loss 2.9948 Accuracy 0.6100\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train0/ckpt-2\n",
      "val-dataset result: Loss 3.3163 Accuracy 0.6571\n",
      "Epoch 4 Loss 2.9625 Accuracy 0.6154\n",
      "Time taken for 1 epoch: 1273.81 secs\n",
      "\n",
      "Epoch 5 Batch 100 Loss 2.4282 Accuracy 0.6903\n",
      "Epoch 5 Batch 200 Loss 2.3141 Accuracy 0.7057\n",
      "Epoch 5 Batch 300 Loss 2.2239 Accuracy 0.7197\n",
      "Epoch 5 Batch 400 Loss 2.1742 Accuracy 0.7269\n",
      "Epoch 5 Batch 500 Loss 2.1342 Accuracy 0.7327\n",
      "Epoch 5 Loss 2.1152 Accuracy 0.7359\n",
      "Time taken for 1 epoch: 138.52 secs\n",
      "\n",
      "Epoch 6 Batch 100 Loss 1.7491 Accuracy 0.7840\n",
      "Epoch 6 Batch 200 Loss 1.7049 Accuracy 0.7925\n",
      "Epoch 6 Batch 300 Loss 1.6691 Accuracy 0.7980\n",
      "Epoch 6 Batch 400 Loss 1.6496 Accuracy 0.8011\n",
      "Epoch 6 Batch 500 Loss 1.6292 Accuracy 0.8039\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train0/ckpt-3\n",
      "val-dataset result: Loss 2.4679 Accuracy 0.7667\n",
      "Epoch 6 Loss 1.6218 Accuracy 0.8054\n",
      "Time taken for 1 epoch: 1269.37 secs\n",
      "\n",
      "Epoch 7 Batch 100 Loss 1.5219 Accuracy 0.8210\n",
      "Epoch 7 Batch 200 Loss 1.4568 Accuracy 0.8283\n",
      "Epoch 7 Batch 300 Loss 1.4290 Accuracy 0.8322\n",
      "Epoch 7 Batch 400 Loss 1.4406 Accuracy 0.8309\n",
      "Epoch 7 Batch 500 Loss 1.4399 Accuracy 0.8314\n",
      "Epoch 7 Loss 1.4383 Accuracy 0.8322\n",
      "Time taken for 1 epoch: 137.85 secs\n",
      "\n",
      "Epoch 8 Batch 100 Loss 1.4158 Accuracy 0.8350\n",
      "Epoch 8 Batch 200 Loss 1.3732 Accuracy 0.8412\n",
      "Epoch 8 Batch 300 Loss 1.3754 Accuracy 0.8416\n",
      "Epoch 8 Batch 400 Loss 1.3826 Accuracy 0.8413\n",
      "Epoch 8 Batch 500 Loss 1.3891 Accuracy 0.8413\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train0/ckpt-4\n",
      "val-dataset result: Loss 2.2986 Accuracy 0.7859\n",
      "Epoch 8 Loss 1.3827 Accuracy 0.8427\n",
      "Time taken for 1 epoch: 1269.48 secs\n",
      "\n",
      "Epoch 9 Batch 100 Loss 1.3920 Accuracy 0.8424\n",
      "Epoch 9 Batch 200 Loss 1.3700 Accuracy 0.8439\n",
      "Epoch 9 Batch 300 Loss 1.3245 Accuracy 0.8504\n",
      "Epoch 9 Batch 400 Loss 1.3388 Accuracy 0.8500\n",
      "Epoch 9 Batch 500 Loss 1.3379 Accuracy 0.8500\n",
      "Epoch 9 Loss 1.3360 Accuracy 0.8506\n",
      "Time taken for 1 epoch: 141.11 secs\n",
      "\n",
      "Epoch 10 Batch 100 Loss 1.4028 Accuracy 0.8421\n",
      "Epoch 10 Batch 200 Loss 1.3668 Accuracy 0.8456\n",
      "Epoch 10 Batch 300 Loss 1.3699 Accuracy 0.8472\n",
      "Epoch 10 Batch 400 Loss 1.3694 Accuracy 0.8478\n",
      "Epoch 10 Batch 500 Loss 1.3589 Accuracy 0.8496\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train0/ckpt-5\n",
      "val-dataset result: Loss 2.3809 Accuracy 0.7919\n",
      "Epoch 10 Loss 1.3615 Accuracy 0.8499\n",
      "Time taken for 1 epoch: 1274.31 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_loss_main = []\n",
    "val_loss_main = []\n",
    "train_acc_main = []\n",
    "val_acc_main = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % (EPOCHS//5) == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "        for inp, tar  in val_dataset:\n",
    "            val_step(inp, tar)\n",
    "\n",
    "        print(f'val-dataset result: Loss {val_loss.result():.4f} Accuracy {val_accuracy.result():.4f}')\n",
    "        train_loss_main.append(train_loss.result())\n",
    "        val_loss_main.append(val_loss.result())\n",
    "        train_acc_main.append(train_accuracy.result())\n",
    "        val_acc_main.append(val_accuracy.result())\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "former-found",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtIUlEQVR4nO3deZgU1dn38e8NiIhsYXEDWYwoUZFtRMUNt1dUHggEEUKio1HiFh+J0aCoEBSNS9wel4gaUBgFNUowokZxDy6MBhcQFAkIKMgiCLIz9/vHqRmaoWemB2amunt+n+vqa7qqTlfddbrm7upTp0+ZuyMiIpmvRtwBiIhIxVBCFxHJEkroIiJZQgldRCRLKKGLiGQJJXQRkSyhhF5BzOxFMzu3osvGyczmm9kplbDeN8zsguj5IDP7Vypld2I7Lc1srZnV3NlYs01UHwfs4jrGmtlNFRWTVJxqndCjg7vwUWBm6xOmB5VnXe5+urs/VtFl05GZDTWzt5LMb2pmm8zssFTX5e557v7/Kiiu7T6A3P1rd6/n7lsrYv0VwczczA6Ma/tRfcyLa/tSuap1Qo8O7nruXg/4GvifhHl5heXMrFZ8Uaal8UA3M2tTbP4A4FN3/yyGmKQU1e0Yrm77W6haJ/SSmFl3M1tkZn80syXAGDP7iZn908yWmdn30fMWCa9JbEbINbN3zOyOqOx/zez0nSzbxszeMrM1Zvaqmd1vZuNLiDuVGG80s39H6/uXmTVNWP5rM1tgZivMbFhJ9ePui4DXgF8XW3QO8HhZcRSLOdfM3kmYPtXMZpvZajO7D7CEZT81s9ei+JabWZ6ZNYqWjQNaAs9H37CuNrPW0RlxrajMfmY22cxWmtlcM7swYd0jzOwpM3s8qpuZZpZTUh2Y2aFm9kq0rqVmdm00v6uZvWtmq8zsWzO7z8xqR8sKv9V8HMV4djS/p5nNiF4zzcwOT9hOZzP7TxTT02Y20RKaO8zswmhfVkb7tl/CMjezS83sS+DLhHkHRs/3MLO/RO/56ug43CNa9rSZLYnmv2Vmh5ZUFyXUT1nHYmMzG2Nm30TLJyUs6x3Vxw9m9pWZ9Yjmb/cNLHrPxkfPC9/r35jZ14Tjs9T9KGn/zewFM/tdsf35xMz6lKcO4qCEXrJ9gMZAK2Awoa7GRNMtgfXAfaW8/khgDtAUuA141MxsJ8o+AXwANAFGsGMSTZRKjL8EzgP2AmoDfwAws0OAB6P17xdtL2kSjjyWGIuZHQx0jOItb10VrqMp8CxwHaEuvgKOSSwC3BLF9zNgf0Kd4O6/ZvtvWbcl2cQEYFH0+n7AzWZ2UsLyXlGZRsDkkmI2s/rAq8BL0boOBKZGi7cCQ6L4jwZOBi6JYjw+KtMhinGimXUC/gb8llDnDwGTzWz36IPgOWAs4Vh8EihKKlHstwD9gX2BBVH8iX5OOL4OSbIrdwBdgG7R+q8GCqJlLwJtCcfJR0BekteXpqxjYBxQFzg02sZd0T51BR4HriK8D8cD88ux3RMIx8ZpKexHSfv/GPCrwkJm1gFoDrxQjjji4e56hPFs5gOnRM+7A5uAOqWU7wh8nzD9BnBB9DwXmJuwrC7gwD7lKUv4R9gC1E1YPh4Yn+I+JYvxuoTpS4CXouc3ABMSlu0Z1cEpJay7LvAD0C2aHgX8Yyfr6p3o+TnAewnljJCALyhhvT8H/pPsPYymW0d1WYuQ/LcC9ROW3wKMjZ6PAF5NWHYIsL6E7Q5M3G4Z78EVwHMJ0w4cmDD9IHBjsdfMISSm44HFgCUsewe4KXr+KHBbwrJ6wGagdcK2Tiq2bid8ANUgJNkOKexDo+h1DaPpsYUxlOP/q+gYIHz4FAA/SVLuIeCuEtZR/P0dQfS/kPBeH5DKfpS2/0Ad4HugbTR9B/BAefY3rofO0Eu2zN03FE6YWV0zeyj6evYD8BbQyEruQbGk8Im7r4ue1itn2f2AlQnzABaWFHCKMS5JeL4uIab9Etft7j8CK0raVhTT08A50beJQYQzq52pq0LFY/DEaTPb28wmmNniaL3jCWfCqSisyzUJ8xYQzrwKFa+bOpa8LXZ/wreHHZjZQVHzwpIoxpvLiLEVcGXU3LLKzFZF698veiyO6qFQ4vu/X7QPALj7WsJ71ryE8omaEhLXDvthZjXN7M9Rc8cPbDtDTrWuyzoG9ie8F98neWmJdZuixOOltP0ocf+j//uJwK/MrAbhA3zcLsRUZZTQS1Z8GMorgYOBI929AeHsCRLaeCvBt0BjM6ubMG//UsrvSozfJq472maTMl7zGOHr/qlAfeD5XYyjeAzG9vt7M+F9aR+t91fF1lna0KHfEOqyfsK8loQz4PJaCJTU9e9BYDbh7K4BcC2l7/dCYJS7N0p41HX3Jwn10bxYU11ifXxD+EAAwMz2JLxniftUUp0sBzYAP02y7JdAb+AUwtls68JNlLIfxZV2DCwkvBeNkrxuYQkxAfxI+GZYaJ8kZRL3t7T9KG3/IRzbgwhNZuvc/d0SyqUVJfTU1Sd8RVtlZo2B4ZW9QXdfAOQDI8ystpkdDfxPJcX4DNDTzI6N2m5HUvbx8TawChhNaK7ZtItxvAAcamZ9ozPjy9n+n7Y+sBZYbWbNCe2siZZSQqJ194XANOAWM6tj4cLjbwhn+eX1T2BfM7siauuub2ZHJsT4A7DWzNoBF5cR48PARWZ2pAV7mtmZ0QfPu4RmosvMrJaZ9Qa6Jrz2SeA8M+toZrsTPvDed/f5Ze2AuxcQ2u7vtHCxuKaZHR2tpz6wkXC2Xzdab3mVeAy4+7eEtu0HLFw83c3MChP+o9E+nWxmNcyseVSPADOAAVH5HMJ1kLJiSLofZew/UQIvAP5ChpydgxJ6edwN7EH4ZH+PcEGsKgwiXFxbAdxE+Cq4sYSyd7OTMbr7TOBSwkXNbwltiIvKeI0TmllaRX93KQ53Xw6cBfyZsL9tgX8nFPkT0BlYTUj+zxZbxS3AdVHTxR+SbGIg4SztG8LFxuHu/moqsRWLcw3hW8n/EJppvgROjBb/gXBmuIaQrCcWe/kI4LEoxv7ung9cSLhg+D0wl3BdgegDsi/hg2cV4RvJP4ne/yj264G/E96znxK6jqbqD8CnwHRgJXArISc8TmjKWQzMIryH5XU3pR8Dvya0988GviNca8DdPyBctL+L8D6/ybZvIdcT9vF7wrHwRBkxlLUfJe1/4uvbs3Mf+rGw7ZvnJN2Z2URgtrtX+jcEST9m9j7wV3cfE3cs2c7MzgEGu/uxcceSKp2hpzkzO8JC/+saFvrj9gYmxRyWVBEzO8HM9omaXM4FDqfqvh1WW9E1pEsIzYkZQwk9/e1D6Oa3FrgXuNjd/xNrRFKVDgY+JjS5XAn0i9qgY2dm19r2w2cUPl6MO7ZdYWanAcsI1zvKatZJK2pyERHJEjpDFxHJErENYNO0aVNv3bp1XJsXEclIH3744XJ3b5ZsWWwJvXXr1uTn58e1eRGRjGRmC0papiYXEZEsoYQuIpIllNBFRLJEWt3VY/PmzSxatIgNGzaUXViqhTp16tCiRQt22223uEMRSXtpldAXLVpE/fr1ad26NVbivSCkunB3VqxYwaJFi2jTpvjd7kSkuLRqctmwYQNNmjRRMhcAzIwmTZroG5tkjbw8aN0aatQIf/PKex+oMqTVGTqgZC7b0fEg2SIvDwYPhnXR7WoWLAjTAIMGVcw20uoMXUQkWw0bti2ZF1q3LsyvKEroCVasWEHHjh3p2LEj++yzD82bNy+a3rRpU6mvzc/P5/LLLy9zG926dauocEViV9lNCNnk66/LN39nZHRCr+iDqUmTJsyYMYMZM2Zw0UUXMWTIkKLp2rVrs2XLlhJfm5OTw7333lvmNqZNm7ZrQcZg69atcYcgaaiwCWHBAnDf1oSgpJ5cy5blm78zMjahV9XBlJuby0UXXcSRRx7J1VdfzQcffMDRRx9Np06d6NatG3PmzAHgjTfeoGfPngCMGDGC888/n+7du3PAAQdsl+jr1atXVL579+7069ePdu3aMWjQoMI7jjNlyhTatWtHly5duPzyy4vWm2j+/Pkcd9xxdO7cmc6dO2/3QXHrrbfSvn17OnTowNChQwGYO3cup5xyCh06dKBz58589dVX28UMcNlllzF27FggDM3wxz/+kc6dO/P000/z8MMPc8QRR9ChQwd+8YtfsC767rh06VL69OlDhw4d6NChA9OmTeOGG27g7rvvLlrvsGHDuOeee3b1rZA0UxVNCNlk1CioW3f7eXXrhvkVxt1jeXTp0sWLmzVr1g7zStKqlXtI5ds/WrVKeRWlGj58uN9+++1+7rnn+plnnulbtmxxd/fVq1f75s2b3d39lVde8b59+7q7++uvv+5nnnlm0WuPPvpo37Bhgy9btswbN27smzZtcnf3Pffcs6h8gwYNfOHChb5161Y/6qij/O233/b169d7ixYtfN68ee7uPmDAgKL1Jvrxxx99/fr17u7+xRdfeGF9TpkyxY8++mj/8ccf3d19xYoV7u7etWtXf/bZZ93dff369f7jjz9uF7O7+6WXXupjxoxxd/dWrVr5rbfeWrRs+fLlRc+HDRvm9957r7u79+/f3++66y53d9+yZYuvWrXK//vf/3qnTp3c3X3r1q1+wAEHbPf68irPcSFVxyz5/6BZ3JGlr/HjQ44yC3/Hjy//OoB8LyGvpl0vl1RVRXtUobPOOouaNWsCsHr1as4991y+/PJLzIzNmzcnfc2ZZ57J7rvvzu67785ee+3F0qVLadGixXZlunbtWjSvY8eOzJ8/n3r16nHAAQcU9bseOHAgo0fveNOUzZs3c9lllzFjxgxq1qzJF198AcCrr77KeeedR93oVKBx48asWbOGxYsX06dPHyD8WCcVZ599dtHzzz77jOuuu45Vq1axdu1aTjvtNABee+01Hn883E60Zs2aNGzYkIYNG9KkSRP+85//sHTpUjp16kSTJk1S2qZkjpYtwzfjZPMluUGDKq5HSzIZ2+RSFe1Rhfbcc8+i59dffz0nnngin332Gc8//3yJfaR33333ouc1a9ZM2v6eSpmS3HXXXey99958/PHH5Ofnl3nRNplatWpRUFBQNF18XxL3Ozc3l/vuu49PP/2U4cOHl9k3/IILLmDs2LGMGTOG888/v9yxSfqrkiYEKZeMTehxHUyrV6+mefPmAEXtzRXp4IMPZt68ecyfPx+AiROL3zR+Wxz77rsvNWrUYNy4cUUXLk899VTGjBlT1Ma9cuVK6tevT4sWLZg0aRIAGzduZN26dbRq1YpZs2axceNGVq1axdSpU0uMa82aNey7775s3ryZvIQLFSeffDIPPvggEC6erl69GoA+ffrw0ksvMX369KKzeckugwbB6NHQqhWYhb+jR1fuGaiULmMTelwH09VXX80111xDp06dynVGnao99tiDBx54gB49etClSxfq169Pw4YNdyh3ySWX8Nhjj9GhQwdmz55ddDbdo0cPevXqRU5ODh07duSOO+4AYNy4cdx7770cfvjhdOvWjSVLlrD//vvTv39/DjvsMPr370+nTp1KjOvGG2/kyCOP5JhjjqFdu3ZF8++55x5ef/112rdvT5cuXZg1axYAtWvX5sQTT6R///5FzVWSfQYNgvnzoaAg/FUyj1ds9xTNycnx4je4+Pzzz/nZz34WSzzpZO3atdSrVw9359JLL6Vt27YMGTIk7rDKpaCgoKiHTNu2bXdpXTouRLYxsw/dPSfZsow9Q89mDz/8MB07duTQQw9l9erV/Pa3v407pHKZNWsWBx54ICeffPIuJ3MRSV3G9nLJZkOGDMm4M/JEhxxyCPPmzYs7DJFqR2foIiJZQgldRCRLKKGLiGQJJXQRkSyhhJ7gxBNP5OWXX95u3t13383FF19c4mu6d+9OYffLM844g1WrVu1QZsSIEUX9wUsyadKkoj7cADfccAOvvvpqOaKXXaWhYCXTKaEnGDhwIBMmTNhu3oQJExg4cGBKr58yZQqNGjXaqW0XT+gjR47klFNO2al1xSWTh9nVULCSDVJK6GbWw8zmmNlcMxuaZHlLM3vdzP5jZp+Y2RkVH2rl69evHy+88ELRuCjz58/nm2++4bjjjuPiiy8mJyeHQw89lOHDhyd9fevWrVm+fDkAo0aN4qCDDuLYY48tGmIXSDoM7bRp05g8eTJXXXUVHTt25KuvviI3N5dnnnkGgKlTp9KpUyfat2/P+eefz8aNG4u2N3z4cDp37kz79u2ZPXv2DjFpmN3UaChYyQZl9kM3s5rA/cCpwCJguplNdvdZCcWuA55y9wfN7BBgCtB6VwK74gqYMWNX1rCjjh0hIX/soHHjxnTt2pUXX3yR3r17M2HCBPr374+ZMWrUKBo3bszWrVs5+eST+eSTTzj88MOTrufDDz9kwoQJzJgxgy1bttC5c2e6dOkCQN++fbnwwgsBuO6663j00Uf53e9+R69evejZsyf9+vXbbl0bNmwgNzeXqVOnctBBB3HOOefw4IMPcsUVVwDQtGlTPvroIx544AHuuOMOHnnkke1ev9dee/HKK69Qp04dvvzySwYOHEh+fj4vvvgi//jHP3j//fepW7cuK1euBGDQoEEMHTqUPn36sGHDBgoKCli4cGGp9dqkSRM++ugjINz1Kdn+XX755Zxwwgk899xzbN26lbVr17LffvvRt29frrjiCgoKCpgwYQIffPBBqduqLFU5eqdIZUnlDL0rMNfd57n7JmAC0LtYGQcaRM8bAt9UXIhVK7HZJbG55amnnqJz58506tSJmTNnbtc8Utzbb79Nnz59qFu3Lg0aNKBXr15Fyz777DOOO+442rdvT15eHjNnziw1njlz5tCmTRsOOuggAM4991zeeuutouV9+/YFoEuXLkUDeiXavHkzF154Ie3bt+ess84qijvVYXbrFh8BLYniw+wm27/XXnut6FpE4TC7rVu3Lhpm91//+lesw+xW5eidIpUllV+KNgcST9EWAUcWKzMC+JeZ/Q7YE0ja+Gtmg4HBAC3L+E8p7Uy6MvXu3ZshQ4bw0UcfsW7dOrp06cJ///tf7rjjDqZPn85PfvITcnNzyxw+tiS5ublMmjSJDh06MHbsWN54441dirdwCN6Sht9NHGa3oKAg5bHQE5V3mN3y7F/hMLtLliyJdZjdUaO2vyM7aChYyTwVdVF0IDDW3VsAZwDjzGyHdbv7aHfPcfecZs2aVdCmK1a9evU48cQTOf/884vOzn/44Qf23HNPGjZsyNKlS3nxxRdLXcfxxx/PpEmTWL9+PWvWrOH5558vWlbSMLT169dnzZo1O6zr4IMPZv78+cydOxcIoyaecMIJKe+PhtlNjYaClWyQSkJfDOyfMN0impfoN8BTAO7+LlAHaFoRAcZh4MCBfPzxx0UJvUOHDnTq1Il27drxy1/+kmOOOabU13fu3Jmzzz6bDh06cPrpp3PEEUcULStpGNoBAwZw++2306lTJ7766qui+XXq1GHMmDGcddZZtG/fnho1anDRRRelvC8aZjd1GgpWKos7bN4Ma9bAsmXw44+Vs50yh881s1rAF8DJhEQ+Hfilu89MKPMiMNHdx5rZz4CpQHMvZeUaPlcgtWF2dVxIRXCHjRthwwZYvz78TfYoaVl55xdfltBqyUMPhSa+nVHa8LlltqG7+xYzuwx4GagJ/M3dZ5rZSMLNSicDVwIPm9kQwgXS3NKSuQiEYXZ79uxJnz59NMxuNbF1a8Un1FRfE/X23WlmUKdOeOyxx7bniY9GjZLPL/6aMr7k77SUhs919ymEroiJ825IeD4LqKQQJVtpmN3MsmYNfPPNjo8lS0ITQiqJdldv8rXbbqUnzAYNYK+9Sk+mqcxPtmy33UJST2dpNx66u2PpXmtSZfRFr/KtXw/ffps8WRc+Fi+GtWt3fO2ee8K++0L9+tsSX9OmO5cwy1q2++5QK+0yVnpJq+qpU6cOK1asoEmTJkrqgruzYsWKnepqKeEi3JIlZSfq77/f8bW77w777QfNm0OHDnD66WG6+KN+/arfLylZWiX0Fi1asGjRIpYtWxZ3KJIm6tSpQ4sWLeIOI61s3Rp6SpSUoAufL1sWLgQmqlUrnFHvtx+0bQsnnJA8Uf/kJ+nfvCA7SquEvttuu9GmTZu4wxCJhTusXFl6ki5ssy4+DpoZ7L13SMYtWkDXrskTdbNmYTRJyU5pldBFspH7jhcUiyfpwkc0Ltx2mjTZlpAPOyx5ot5773DRTqo3JXSRXbBu3bYLiiUl6W++Sf5DkgYNtiXkY49Nnqj33TdcEBRJhRK6SBKbNm27oFhaok5yPxPq1AkXE/fbDzp3hp49kydqXVCUiqaELhLZuBHuuSc8vkkyXmitWtsScrt2cNJJyc+qGzXSBUWJhxK6VHvu8MILMGQIzJ0Lp50Gv/3t9km6efPQlq0LipLOlNClWps9OyTyl14KZ90vvRQSukgm0vmGVEurV8OVV0L79jBtGtx5J3zyiZK5ZDadoUu1UlAAY8bAtdeGH9785jfhJhZ77RV3ZCK7Tgldqo1p0+Dyy+HDD6FbN5gyBaJbvYpkBTW5SNZbvBh+9aswZOmSJZCXB++8o2Qu2Udn6JK1NmwIbeM33xyGbR02DIYOhXr14o5MpHIooUvWcYd//CNc9Jw3D37+c/jLX+CAA+KOTKRyqclFssqsWaGnSp8+YUztV16B555TMpfqQQldssL338P//i8cfjhMnw733gszZsApp8QdmUjVUZOLZLStW+HRR0P7+IoV4ReeN94Y7pojUt3oDF0y1ttvwxFHhCT+s5/BRx/Bgw8qmUv1pYQuGWfhQhg4EI4/HpYvhwkT4M03oWPHuCMTiZeaXCRjrF8Pd9wBt9wSerLccAP88Y9Qt27ckYmkByV0SXvu8OyzoRviggXQr19I7K1axR2ZSHpRk4uktU8/hZNPDkm8QQN47TV4+mklc5FklNAlLa1cCZddFtrFP/4Y7r8/XPQ88cS4IxNJX2pykbSyZQuMHg3XXx9u73bxxTByJDRuHHdkIulPCV3SxhtvhNEQP/00nInfc08Yr1xEUqMmF4ndggVw1lkhif/wAzzzDEydqmQuUl46Q5fYrFsHt94Kt90Wbqo8ciT84Q9hDBYRKT8ldKly7vDUU3DVVeFHQgMGhKS+//5xRyaS2dTkIlVqxgzo3j0k8SZN4K234MknlcxFKoISulSJ5cvhoovCXYJmzYKHHoL8fDjuuLgjE8keKSV0M+thZnPMbK6ZDU2y/C4zmxE9vjCzVRUeqWSkzZvDULZt28Ijj8DvfgdffAGDB0PNmnFHJ5JdymxDN7OawP3AqcAiYLqZTXb3WYVl3H1IQvnfAZ0qIVbJMK++GsYonzULTj0V7r4bDjkk7qhEslcqZ+hdgbnuPs/dNwETgN6llB8IPFkRwUlmmjcv3DHo1FPDfT0nTYKXX1YyF6lsqST05sDChOlF0bwdmFkroA3wWgnLB5tZvpnlL1u2rLyxSppbuxauuy4k7ldeCTdnnjkTevcO3RJFpHJV9EXRAcAz7r412UJ3H+3uOe6e06xZswretMTFHfLyoF07GDUq/Ehozhy45hqoUyfu6ESqj1QS+mIgsVNZi2heMgNQc0u18tFHoafKr34F++wD//43jBsHzZN+hxORypRKQp8OtDWzNmZWm5C0JxcvZGbtgJ8A71ZsiJKOvvsOLrwQcnLgyy/DfT0/+AC6dYs7MpHqq8yE7u5bgMuAl4HPgafcfaaZjTSzXglFBwAT3N0rJ1RJB5s3w113wUEHwdix8Pvfh26I558PNfSrBpFYpfTTf3efAkwpNu+GYtMjKi4sSUcvvwxXXAGzZ0OPHqEb4sEHxx2ViBTSOZWUae5c6NUrJPEtW+D552HKFCVzkXSjhC4lWrMGhg6FQw+F118PIyN+9hn07KluiCLpSKMtyg4KCmD8+JDMv/0WcnPhlltCLxYRSV9K6LKdDz4Idw16/33o2hWeew6OPDLuqEQkFWpyEQCWLIHzzgvJe8GC0IPl3XeVzEUyiRJ6NbdpE9x+e+iGmJcHV18duiGee666IYpkGjW5VGMvvABDhoQfBvXsCXfeGYa5FZHMpHOwamjOHDjjjJDEa9SAF18MXRGVzEUymxJ6NfLDD+EmzIcdFsZc+ctf4JNPQv9yEcl8anKpBgoKwkXOa66BZcvCz/RHjYK99447MhGpSEroWW7evHBD5unTw8BZU6aE+3qKSPZRQs9iK1fC6aeHs/Lx4+GXv9QvPEWymRJ6ltq0Cfr2hfnzYepUOPbYuCMSkcqmhJ6F3MNY5W++CU88oWQuUl2ol0sWuukmePxxGDkSBg6MOxoRqSpK6FnmiSfghhvgnHPCDZtFpPpQQs8i77wTxmM54QR4+GFdABWpbpTQs8TcufDzn0Pr1vDss1C7dtwRiUhVU0LPAitWhJ/yQ+hn3rhxvPGISDzUyyXDbdwYuicuWACvvQY//WncEYlIXJTQM1hh98S33goXQ485Ju6IRCROanLJYDfeCOPGhb/qnigiSugZKi8Phg8PN6IYNizuaEQkHSihZ6C33w4jJnbvDqNHq3uiiARK6Bnmyy+hTx9o0wb+/nd1TxSRbZTQM8iKFXDmmeGM/IUXyu6emJcX+qXXqBH+5uVVRZQiEhf1cskQGzeGM/Ovvw6jJ5bVPTEvDwYPhnXrwvSCBWEaYNCgyo1VROKhM/QM4A4XXBDazseOTa174rBh25J5oXXrdAFVJJspoWeAkSPDDSpuuincfSgVX39dvvkikvmU0NPc+PEwYgTk5sK116b+upYtyzdfRDKfEnoae+st+M1v4MQT4aGHytc9cdQoqFt3+3l164b5IpKdUkroZtbDzOaY2VwzG1pCmf5mNsvMZprZExUbZvWzq90TBw0KfdRbtQofBK1ahWldEBXJXubupRcwqwl8AZwKLAKmAwPdfVZCmbbAU8BJ7v69me3l7t+Vtt6cnBzPz8/f1fiz0ooVcNRRsGoVvP8+HHBA3BGJSLowsw/dPSfZslTO0LsCc919nrtvAiYAvYuVuRC4392/BygrmUvJNm4M45ovXAj/+IeSuYikLpWE3hxYmDC9KJqX6CDgIDP7t5m9Z2Y9kq3IzAabWb6Z5S9btmznIs5i7qHN/J134LHHoFu3uCMSkUxSURdFawFtge7AQOBhM2tUvJC7j3b3HHfPadasWQVtOnv86U/hB0GjRsHZZ8cdjYhkmlQS+mJg/4TpFtG8RIuAye6+2d3/S2hzb1sxIVYP48aFhH7eeXDNNXFHIyKZKJWEPh1oa2ZtzKw2MACYXKzMJMLZOWbWlNAEM6/iwsxuid0T//pXjZ4oIjunzITu7luAy4CXgc+Bp9x9ppmNNLNeUbGXgRVmNgt4HbjK3VdUVtDZ5IsvwkXQn/5UoyeKyK4ps9tiZVG3RVi+HI4+GlavhvfeU48WESlbad0WNdpiTApHT1y4EF5/XclcRHadEnoM3MMdh955ByZODGfpIiK7SmO5xGDECHjiCbj5ZujfP+5oRCRbKKFXsccfD8Phnn8+DE06Ko6IyM5RQq9Cb74ZblRx0knqnigiFU8JvYrMmRMugh54YOieuNtucUckItlGCb0KLF8ebu5cq1a4uXOjRnFHJCLZSL1cKtmGDeGHQ4sXh+6JbdrEHZGIZCsl9EpU2D3x3/+Gp54KY5yLiFQWNblUouHD4ckn4ZZb4Kyz4o5GRLKdEnoleewxuPHGMOjWH/8YdzQiUh0ooVeCN96ACy+Ek0+GBx9U90QRqRpK6BVszhzo2zd0T3zmGXVPFJGqo4RegZYtgzPOUPdEEYmHerlUkMLuid98o+6JIhIPJfQKUFAQbh03bRo8/bS6J4pIPNTkUgGGD4cJE+DPf4Z+/eKORkSqKyX0XTR2LNx0Uxh06+qr445GRKozJfRd8PrrMHgwnHIKPPCAuieKSLyU0HfS7Nmhe2LbtqHdXN0TRSRuSug7obB7Yu3a6p4oIulDvVzKacMG6N0bvv02/CK0deu4IxIRCZTQy6GgAHJz4d13w69Ajzwy7ohERLZRk0s53HADTJwIt94Kv/hF3NGIiGxPCT1FY8bAqFFh0K2rroo7GhGRHSmhp+C110L3xFNPhfvvV/dEEUlPSuhl+Pzz0Lxy0EHqnigi6U0JvRTffRdu7lzYPbFhw7gjEhEpmXq5lGD9+jB64rffwptvqnuiiKQ/JfQkCrsnvvdeaGbp2jXuiEREyqaEnsT118NTT8Ftt6l7oohkjpTa0M2sh5nNMbO5ZjY0yfJcM1tmZjOixwUVH2rV+Nvf4OabQ6+WP/wh7mhERFJX5hm6mdUE7gdOBRYB081ssrvPKlZ0ortfVgkxVpmpU+G3vw3dE++7T90TRSSzpHKG3hWY6+7z3H0TMAHoXblhVb3C7okHH6zuiSKSmVJJ6M2BhQnTi6J5xf3CzD4xs2fMbP8Kia6KfPddGD2xTh11TxSRzFVR/dCfB1q7++HAK8BjyQqZ2WAzyzez/GXLllXQpnfN+vVh9MSlS2HyZGjVKu6IRER2TioJfTGQeMbdIppXxN1XuPvGaPIRoEuyFbn7aHfPcfecZs2a7Uy8FaqgAM49F95/H/Ly1D1RRDJbKgl9OtDWzNqYWW1gADA5sYCZ7Zsw2Qv4vOJCrDzDhoX28ttugz594o5GRGTXlNnLxd23mNllwMtATeBv7j7TzEYC+e4+GbjczHoBW4CVQG4lxlwhHn0U/vzn0KvlyivjjkZEZNeZu8ey4ZycHM/Pz49l21OnQo8ecNJJ8M9/qkeLiGQOM/vQ3XOSLat2g3PNmhW6J7ZrF34NqmQuItmiWiX0pUvD6Il16oQzc3VPFJFsUm3Gcknsnvjmm+qeKCLZp1ok9IICOOcc+OAD+Pvf4Ygj4o5IRKTiVYuEfu218MwzcMcd6p4oItkr69vQH3kEbr0VLroIfv/7uKMREak8WZ3QX301JPLTToP/+z+Nnigi2S1rE/rMmaF74iGHhO6JtapF45KIVGdZmdALuyfWrRu6JzZoEHdEIiKVL+vOW9etg169wpC4b70FLVvGHZGISNXIqoRe2D1x+nR49lnISfrjWBGR7JRVCf2aa0I/87/8BX7+87ijERGpWlnThv7ww2EY3IsvhiFD4o5GRKTqZUVCf+WVkMh79IB771X3RBGpnjI+oX/2GfTrF7onTpyo7okiUn1ldEJfsgR69lT3RBERyOCLooXdE5ctU/dEERHI0IReUAC//jXk58Nzz0GXpLekFhGpXjIyoQ8dGvqZ33lnGONcREQysA394Yfh9tvhkkvgiivijkZEJH1kXELv2BFyc+Gee9Q9UUQkUcY1uRxxBIwZE3cUIiLpJ+PO0EVEJDkldBGRLKGELiKSJZTQRUSyhBK6iEiWUEIXEckSSugiIllCCV1EJEsooYuIZAkldBGRLJFSQjezHmY2x8zmmtnQUsr9wszczHIqLkQREUlFmQndzGoC9wOnA4cAA83skCTl6gP/C7xf0UGKiEjZUjlD7wrMdfd57r4JmAAkG4X8RuBWYEMFxiciIilKJaE3BxYmTC+K5hUxs87A/u7+QmkrMrPBZpZvZvnLli0rd7AiIlKyXb4oamY1gDuBK8sq6+6j3T3H3XOaNWu2q5sWEZEEqST0xcD+CdMtonmF6gOHAW+Y2XzgKGCyLoyKiFStVBL6dKCtmbUxs9rAAGBy4UJ3X+3uTd29tbu3Bt4Derl7fqVELCIiSZWZ0N19C3AZ8DLwOfCUu880s5Fm1quyAxQRkdSkdAs6d58CTCk274YSynbf9bBERKS89EtREZEsoYQuIpIllNBFRLKEErqISJZQQhcRyRJK6CIiWUIJXUQkS2RUQs/Lg9atoUaN8DcvL+6IRETSR0o/LEoHeXkweDCsWxemFywI0wCDBsUXl4hIusiYM/Rhw7Yl80Lr1oX5IiKSQQn966/LN19EpLrJmITesmX55ouIVDcZk9BHjYK6dbefV7dumC8iIhmU0AcNgtGjoVUrMAt/R4/WBVERkUIZ08sFQvJWAhcRSS5jztBFRKR0SugiIllCCV1EJEsooYuIZAkldBGRLGHuHs+GzZYBC3by5U2B5RUYTkVRXOWjuMovXWNTXOWzK3G1cvdmyRbEltB3hZnlu3tO3HEUp7jKR3GVX7rGprjKp7LiUpOLiEiWUEIXEckSmZrQR8cdQAkUV/korvJL19gUV/lUSlwZ2YYuIiI7ytQzdBERKUYJXUQkS6RtQjezv5nZd2b2WQnLzczuNbO5ZvaJmXVOk7i6m9lqM5sRPW6oorj2N7PXzWyWmc00s/9NUqbK6yzFuKq8zsysjpl9YGYfR3H9KUmZ3c1sYlRf75tZ6zSJK9fMliXU1wWVHVfCtmua2X/M7J9JllV5faUYV5z1Nd/MPo22m59kecX+T7p7Wj6A44HOwGclLD8DeBEw4Cjg/TSJqzvwzxjqa1+gc/S8PvAFcEjcdZZiXFVeZ1Ed1Iue7wa8DxxVrMwlwF+j5wOAiWkSVy5wX1UfY9G2fw88kez9iqO+UowrzvqaDzQtZXmF/k+m7Rm6u78FrCylSG/gcQ/eAxqZ2b5pEFcs3P1bd/8oer4G+BxoXqxYlddZinFVuagO1kaTu0WP4j0EegOPRc+fAU42M0uDuGJhZi2AM4FHSihS5fWVYlzprEL/J9M2oaegObAwYXoRaZAoIkdHX5lfNLNDq3rj0VfdToSzu0Sx1lkpcUEMdRZ9TZ8BfAe84u4l1pe7bwFWA03SIC6AX0Rf0Z8xs/0rO6bI3cDVQEEJy2OprxTignjqC8KH8b/M7EMzG5xkeYX+T2ZyQk9XHxHGWugA/B8wqSo3bmb1gL8DV7j7D1W57dKUEVcsdebuW929I9AC6Gpmh1XFdsuSQlzPA63d/XDgFbadFVcaM+sJfOfuH1b2tsojxbiqvL4SHOvunYHTgUvN7PjK3FgmJ/TFQOInbYtoXqzc/YfCr8zuPgXYzcyaVsW2zWw3QtLMc/dnkxSJpc7KiivOOou2uQp4HehRbFFRfZlZLaAhsCLuuNx9hbtvjCYfAbpUQTjHAL3MbD4wATjJzMYXKxNHfZUZV0z1VbjtxdHf74DngK7FilTo/2QmJ/TJwDnRVeKjgNXu/m3cQZnZPoXthmbWlVDHlZ4Eom0+Cnzu7neWUKzK6yyVuOKoMzNrZmaNoud7AKcCs4sVmwycGz3vB7zm0ZWsOOMq1sbai3BdolK5+zXu3sLdWxMueL7m7r8qVqzK6yuVuOKor2i7e5pZ/cLnwP8DiveOq9D/ybS9SbSZPUno/dDUzBYBwwkXiHD3vwJTCFeI5wLrgPPSJK5+wMVmtgVYDwyo7IM6cgzwa+DTqP0V4FqgZUJscdRZKnHFUWf7Ao+ZWU3CB8hT7v5PMxsJ5Lv7ZMIH0Tgzm0u4ED6gkmNKNa7LzawXsCWKK7cK4koqDeorlbjiqq+9geeic5VawBPu/pKZXQSV8z+pn/6LiGSJTG5yERGRBEroIiJZQgldRCRLKKGLiGQJJXQRkSyhhC4ikiWU0EVEssT/B+KJ/9P+z607AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(val_acc_main) + 1)\n",
    "\n",
    "plt.plot(epochs, train_acc_main, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc_main, 'b', label = 'Validation accuracy')\n",
    "plt.title('Training and Validation categorical_accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_loss_main, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss_main, 'b', label = 'Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "satellite-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer.save_weights(\"model/transformer_QA_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-malawi",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-caribbean",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
